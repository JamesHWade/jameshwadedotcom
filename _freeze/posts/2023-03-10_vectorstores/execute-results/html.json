{
  "hash": "7ebcf37cf4ef4ba7263c11c12a7f9497",
  "result": {
    "markdown": "---\ntitle: Teaching ChatGPT What It Doesn't Know\nauthor: James H Wade\ndate: 2023-03-25\ndescription: Using a retriever (or vector database) to provide missing context to ChatGPT, similar to ChatGPT Retrieval plugin\nimage: images/retriever.jpg\nexecute: \n  freeze: auto\n  eval: false\nformat: \n  html:\n    toc: true\n    code-copy: true\n    code-link: true\ncategories: \n  - ChatGPT\n  - LLM\n  - NLP\n  - Web Scraping\n  - R\n  - OpenAI\n---\n\n\nLarge language models like GPT-3 and ChatGPT don’t need much of an introduction. They are enormously powerful with benchmarks being surpassed [nearly as quickly as they are created](https://arxiv.org/pdf/2104.14337.pdf). Despite the unprecedented model performance, these models struggle to provide accurate results when the appropriate response requires context more recent than the training data for a model. Vector databases can created from data sources outside the training corpus can address this gap by providing missing context to a model.\n\nVector databases can be use in semantic search with **embeddings** created form source text. By creating embeddings of text data and storing them in a database, we can quickly search for related documents and even perform advanced operations like similarity searches or clustering. This can be especially helpful when working with text data that is more context-dependent or domain-specific, such as scientific or technical documentation.\n\n`{gpttools}` provides a set of tools for working with GPT-3 and other OpenAI models, including the ability to generate embeddings, perform similarity searches, and build vector databases. This package also has convenience functions to aid in scraping web pages to collect text data, generate embeddings, and store those embeddings in a vector database for future use.\n\nTo demonstrate the power of vector databases, we’ll use `{gpttools}` to build a vector database from [_R for Data Science_](https://r4ds.hadley.nz/). The approach uses semantic search to find the most relevant text from the book and then uses ChatGPT to generate a response based on that text via the recently release ChatGPT API.\n\nPopular python packages such as [`llama-index`](https://gpt-index.readthedocs.io/en/latest/index.html) and [`langchain`](https://langchain.readthedocs.io/en/latest/index.html) provide easy utility functions to create vector stores for semantic search with a few lines of python code. `{gpttools}` aims to provide similar functionality with R using data frames as the data structure for the vector store.\n\n## Scraping Text from R4DS\n\nThe first step is to scrape the text from the R4DS book. We'll use the `crawl()` function to scrape the text from the book and store it in a data frame. The `crawl()` function uses the `rvest` package to scrape the text from the online book and `{tokenizers}` to split the text into chunks for subsequent processing.\n\nThe code to scrape the data is relatively simple but is unlikely to work on all sites. From some internal testing, it works quite well on `{pkgdown}` and similar documentation sites.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(gpttools)\ncrawl(\"https://r4ds.hadley.nz/\")\n```\n:::\n\n\nUnder the hood there are a few things going on. Here is the annotated function and associated functions:\n\n::: {.panel-tabset}\n\n## crawl\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncrawl <- function(url,\n                  index_create = TRUE,\n                  aggressive = FALSE,\n                  overwrite = FALSE,\n                  num_cores = parallel::detectCores() - 1) {\n  local_domain <- urltools::url_parse(url)$domain\n  withr::local_options(list(\n    cli.progress_show_after = 0,\n    cli.progress_clear = FALSE\n  ))\n  future::plan(future::multisession, workers = num_cores)\n  scraped_data_dir <-\n    file.path(tools::R_user_dir(\"gpttools\", which = \"data\"), \"text\")\n  scraped_text_file <-\n    glue::glue(\"{scraped_data_dir}/{local_domain}.parquet\")\n\n  if (file.exists(scraped_text_file) && rlang::is_false(overwrite)) {\n    cli::cli_abort(\n      c(\n        \"!\" = \"Scraped data already exists for this domain.\",\n        \"i\" = \"Use {.code crawl(<url>, overwrite = TRUE)} to overwrite.\"\n      )\n    )\n  }\n\n  cli_rule(\"Crawling {.url {url}}\")\n  cli_inform(c(\n    \"i\" = \"This may take a while.\",\n    \"i\" = \"Gathering links to scrape\"\n  ))\n  links <-\n    recursive_hyperlinks(local_domain, url, aggressive = aggressive) |>\n    unique()\n  cli_inform(c(\"i\" = \"Scraping validated links\"))\n  scraped_data <-\n    purrr::map(links, \\(x) {\n      if (identical(check_url(x), 200L)) {\n        tibble::tibble(\n          link    = x,\n          text    = paste(scrape_url(x), collapse = \" \"),\n          n_words = tokenizers::count_words(text)\n        )\n      } else {\n        cli::cli_inform(c(\n          \"!\" = \"Skipped {url}\",\n          \"i\" = \"Status code: {status}\"\n        ))\n      }\n    }) |>\n    dplyr::bind_rows() |>\n    dplyr::distinct()\n  arrow::write_parquet(\n    scraped_data,\n    glue(\"text/{local_domain}.parquet\")\n  )\n}\n```\n:::\n\n\n## recursive_hyperlinks\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrecursive_hyperlinks <- function(local_domain,\n                                 url,\n                                 checked_urls = NULL,\n                                 aggressive = FALSE) {\n  links <- url[!(url %in% checked_urls)]\n  if (length(links) < 1) {\n    return(checked_urls)\n  }\n\n  if (aggressive) {\n    domain_pattern <- glue(\"^https?://(?:.*\\\\.)?{local_domain}/?\")\n  } else {\n    domain_pattern <- glue(\"^https?://{local_domain}/?\")\n  }\n\n  checked_urls <- c(checked_urls, links)\n  cli::cli_inform(c(\"i\" = \"Total urls: {length(checked_urls)}\"))\n  links_df <- furrr::future_map(links, get_hyperlinks) |>\n    dplyr::bind_rows() |>\n    dplyr::filter(!stringr::str_detect(link, \"^\\\\.$|mailto:|^\\\\.\\\\.|\\\\#|^\\\\_$\"))\n\n  new_links <-\n    purrr::pmap(as.list(links_df), \\(parent, link) {\n      clean_link <- NULL\n      if (stringr::str_detect(link, paste0(\"^https?://\", local_domain))) {\n        clean_link <- link\n      } else if (stringr::str_detect(link, \"^/[^/]|^/+$|^\\\\./|^[[:alnum:]]\") &&\n        !stringr::str_detect(link, \"^https?://\")) {\n        if (stringr::str_detect(link, \"^\\\\./\")) {\n          clean_link <- stringr::str_replace(link, \"^\\\\./\", \"/\")\n        } else if (stringr::str_detect(link, \"^[[:alnum:]]\")) {\n          clean_link <- glue::glue(\"/\", link)\n        } else {\n          clean_link <- link\n        }\n        clean_link <- glue::glue(\"{parent}{clean_link}\")\n      }\n    }) |>\n    unlist()\n\n  recursive_hyperlinks(local_domain, unique(new_links), checked_urls)\n}\n```\n:::\n\n\n## get_hyperlinks\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_hyperlinks <- function(url) {\n  rlang::check_installed(\"rvest\")\n  status <- httr::GET(url) |> httr::status_code()\n  if (identical(status, 200L)) {\n    tibble::tibble(\n      parent = url,\n      link = rvest::read_html(url) |>\n        rvest::html_nodes(\"a[href]\") |>\n        rvest::html_attr(\"href\") |>\n        unique()\n    )\n  } else {\n    cli::cli_warn(c(\n      \"!\" = \"URL not valid.\",\n      \"i\" = \"Tried to scrape {url}\",\n      \"i\" = \"Status code: {status}\"\n    ))\n  }\n}\n```\n:::\n\n\n## scrape_url\n\n\n::: {.cell}\n\n```{.r .cell-code}\nscrape_url <- function(url) {\n  rlang::check_installed(\"rvest\")\n  exclude_tags <- c(\"style\", \"script\", \"head\", \"meta\", \"link\", \"button\")\n  text <- rvest::read_html(url) |>\n    rvest::html_nodes(xpath = paste(\"//body//*[not(self::\",\n      paste(exclude_tags, collapse = \" or self::\"),\n      \")]\",\n      sep = \"\"\n    )) |>\n    rvest::html_text2() |>\n    remove_new_lines()\n  if (\"You need to enable JavaScript to run this app.\" %in% text) {\n    cli::cli_warn(\"Unable to parse page {url}. JavaScript is required.\")\n    NULL\n  } else {\n    text\n  }\n}\n```\n:::\n\n\n:::\n\n`crawl()` that takes in a single argument, `url`, which is a character string of the URL to be scraped. The function scrapes all hyperlinks within the same domain. The scraped text is processed into a tibble format and saved as a parquet file using the `{arrow}` package into a directory called \"text\" with a filename that includes the local domain extracted earlier.\n\nThe function begins by extracting the local domain of the input URL using the `urltools::url_parse()` function, which returns a parsed URL object, and then extracting the domain component of the object.\n\nThe function then calls another function called `recursive_hyperlinks()` to recursively extract all hyperlinks within the url and validates the links in the process by only keeping urls that return a status code of `200` (i.e., the webpage is accessible).\n\nThe function then loops through each link and scrapes the text from each webpage and creates a tibble with three columns: link, text, and n_words. The link column contains the URL, the text column contains the scraped text, and the n_words column contains the number of words in the scraped text.\n\n## Generating Embeddings\n\nAfter scraping the text data from the R4DS book, the next step is to generate embeddings for each chunk of text. This can be done using the `create_index()` function provided by the `{gpttools}` package. `create_index()` takes in a single argument, `domain`, which should be a character string indicating the domain of the scraped data.\n\nHere's the code to generate embeddings for the R4DS text data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Specify the domain of the scraped data\ndomain <- \"r4ds.hadley.nz\"\n\n# Create embeddings for each chunk of text in the scraped data\nindex <- create_index(domain)\n```\n:::\n\n\n`create_index()` function prepares the scraped data for indexing using the `prepare_scraped_files()` function, which splits the text into chunks and calculates the number of tokens in each chunk. It then calls `add_embeddings()` to generate embeddings for each chunk of text using the OpenAI API. A resulting tibble with embeddings is stored as a feather file using, again using the `{arrow}` package.\n\nHere's the code for the create_index() function and helper functions:\n\n::: {.panel-tabset}\n\n## create_index\n\n::: {.cell}\n\n```{.r .cell-code}\ncreate_index <- function(domain, overwrite = FALSE) {\n  index_dir <-\n    file.path(tools::R_user_dir(\"gpttools\", which = \"data\"), \"index\")\n  index_file <-\n    glue::glue(\"{index_dir}/{domain}.parquet\")\n\n  if (file.exists(index_file) && rlang::is_false(overwrite)) {\n    cli::cli_abort(\n      c(\n        \"!\" = \"Index already exists for this domain.\",\n        \"i\" = \"Use {.code overwrite = TRUE} to overwrite index.\"\n      )\n    )\n  }\n  cli::cli_inform(c(\n    \"!\" = \"You are about to create embeddings for {domain}.\",\n    \"i\" = \"This will use many tokens. Only proceed if you understand the cost.\",\n    \"i\" = \"Read more about embeddings at {.url\n      https://platform.openai.com/docs/guides/embeddings}.\"\n  ))\n  ask_user <- usethis::ui_yeah(\n    \"Would you like to continue with creating embeddings?\"\n  )\n  if (rlang::is_true(ask_user)) {\n    index <-\n      prepare_scraped_files(domain = domain) |>\n      add_embeddings()\n    if (rlang::is_false(dir.exists(index_dir))) {\n      dir.create(index_dir, recursive = TRUE)\n    }\n    arrow::write_parquet(\n      x    = index,\n      sink = index_file\n    )\n  } else {\n    cli_inform(\"No index was created for {domain}\")\n  }\n}\n```\n:::\n\n\n## prepare_scraped_files\n\n::: {.cell}\n\n```{.r .cell-code}\nprepare_scraped_files <- function(domain) {\n  scraped_dir <- tools::R_user_dir(\"gpttools\", which = \"data\")\n  arrow::read_parquet(glue(\"{scraped_dir}/text/{domain}.parquet\")) |>\n    dplyr::mutate(\n      chunks = purrr::map(text, \\(x) {\n        chunk_with_overlap(x,\n          chunk_size = 500,\n          overlap_size = 50,\n          doc_id = domain,\n          lowercase = FALSE,\n          strip_punct = FALSE,\n          strip_numeric = FALSE,\n          stopwords = NULL\n        )\n      })\n    ) |>\n    tidyr::unnest(chunks) |>\n    tidyr::unnest(chunks) |>\n    dplyr::rename(original_text = text) |>\n    dplyr::mutate(n_tokens = tokenizers::count_characters(chunks) %/% 4)\n}\n```\n:::\n\n\n## add_embeddings\n\n\n::: {.cell}\n\n```{.r .cell-code}\nadd_embeddings <- function(index) {\n  index |>\n    dplyr::mutate(\n      embeddings = purrr::map(\n        .x = chunks,\n        .f = create_openai_embedding,\n        .progress = \"Create Embeddings\"\n      )\n    ) |>\n    tidyr::unnest(embeddings)\n}\n```\n:::\n\n\n## create_openai_embedding\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncreate_openai_embedding <-\n  function(input_text,\n           model = \"text-embedding-ada-002\",\n           openai_api_key = Sys.getenv(\"OPENAI_API_KEY\")) {\n    body <- list(\n      model = model,\n      input = input_text\n    )\n    embedding <- query_openai_api(body, openai_api_key, task = \"embeddings\")\n    embedding$usage$total_tokens\n    tibble::tibble(\n      usage = embedding$usage$total_tokens,\n      embedding = embedding$data$embedding\n    )\n  }\n```\n:::\n\n\n:::\n\nThe resulting tibble contains the following columns:\n\n- `link`: URL of the webpage\n- `original_text`: scraped text\n- `n_words`: number of words in the scraped text\n- `chunks`: text split into chunks\n- `usage`: number of tokens used for creating the embedding\n- `embeddings`: embeddings for each chunk of text\n\n## Querying with Embeddings\n\nAfter generating embeddings for each chunk of text, the next step is to query the embeddings to find similar chunks of text. This can be done using the `query_index()` function provided by the `{gpttools}` package. This is a bit of a complicated function, so it's worth taking a look at the code to see how it works.\n\n::: {.panel-tabset}\n\n## query_index\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquery_index <- function(index, query, history, task = \"Context Only\", k = 4) {\n  arg_match(task, c(\"Context Only\", \"Permissive Chat\"))\n\n  query_embedding <- create_openai_embedding(input_text = query) |>\n    dplyr::pull(embedding) |>\n    unlist()\n\n  full_context <- get_top_matches(index, query_embedding, k = k)\n\n  context <-\n    full_context |>\n    dplyr::pull(chunks) |>\n    paste(collapse = \"\\n\\n\")\n\n  instructions <-\n    switch(task,\n      \"Context Only\" =\n        list(\n          list(\n            role = \"system\",\n            content =\n              glue(\n                \"You are a helpful chat bot that answers questions based on the\n                context provided by the user. If the user does not provide\n                context, say \\\"I am not able to answer that question. Maybe\n                try rephrasing your question in a different way.\\\"\\n\\n\n                Context: {context}\"\n              )\n          ),\n          list(\n            role = \"user\",\n            content = glue(\"{query}\")\n          )\n        ),\n      \"Permissive Chat\" =\n        list(\n          list(\n            role = \"system\",\n            content =\n              glue(\n                \"You are a helpful chat bot that answers questions based on the\n                context provided by the user. If the user does not provide\n                context, say \\\"I am not able to answer that question with the\n                context you gave me, but here is my best answer. Maybe\n                try rephrasing your question in a different way.\\\"\\n\\n\n                Context: {context}\"\n              )\n          ),\n          list(\n            role = \"user\",\n            content = glue(\"{query}\")\n          )\n        )\n    )\n\n  cli_inform(\"Embedding...\")\n\n  history <-\n    purrr::map(history, \\(x) if (x$role == \"system\") NULL else x) |>\n    purrr::compact()\n\n  prompt <- c(history, instructions)\n  answer <- gptstudio::openai_create_chat_completion(prompt)\n  list(prompt, full_context, answer)\n}\n```\n:::\n\n\n## get_top_matches\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_top_matches <- function(index, query_embedding, k = 5) {\n  index |>\n    dplyr::mutate(similarity = purrr::map_dbl(embedding, \\(x) {\n      lsa::cosine(query_embedding, unlist(x))\n    })) |>\n    dplyr::arrange(dplyr::desc(similarity)) |>\n    head(k)\n}\n```\n:::\n\n\n## openai_create_chat_completion (from `{gptstudio}`)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nopenai_create_chat_completion <-\n  function(prompt = \"<|endoftext|>\",\n           model = \"gpt-3.5-turbo\",\n           openai_api_key = Sys.getenv(\"OPENAI_API_KEY\"),\n           task = \"chat/completions\") {\n    assert_that(\n      is.string(model),\n      is.string(openai_api_key)\n    )\n\n    if (is.string(prompt)) {\n      prompt <- list(\n        list(\n          role    = \"user\",\n          content = prompt\n        )\n      )\n    }\n\n    body <- list(\n      model = model,\n      messages = prompt\n    )\n\n    query_openai_api(body, openai_api_key, task = task)\n  }\n```\n:::\n\n\n## query_openai_api\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquery_openai_api <- function(body, openai_api_key, task) {\n  arg_match(task, c(\"completions\", \"chat/completions\", \"edits\", \"embeddings\"))\n\n  base_url <- glue(\"https://api.openai.com/v1/{task}\")\n\n  headers <- c(\n    \"Authorization\" = glue(\"Bearer {openai_api_key}\"),\n    \"Content-Type\" = \"application/json\"\n  )\n\n  response <-\n    httr::RETRY(\"POST\",\n      url = base_url,\n      httr::add_headers(headers), body = body,\n      encode = \"json\",\n      quiet = TRUE\n    )\n\n  parsed <- response %>%\n    httr::content(as = \"text\", encoding = \"UTF-8\") %>%\n    jsonlite::fromJSON(flatten = TRUE)\n\n  if (httr::http_error(response)) {\n    cli_alert_warning(c(\n      \"x\" = glue(\"OpenAI API request failed [{httr::status_code(response)}].\"),\n      \"i\" = glue(\"Error message: {parsed$error$message}\")\n    ))\n  }\n  parsed\n}\n```\n:::\n\n\n:::\n\nThe `query_index()` function takes in four arguments: `index`, `query`, `task`, and `k`.\n\n* `index`: The index containing the embeddings to be queried.\n* `query`: The query string to search for similar chunks of text.\n* `task`: The type of task to perform based on the context of the query. {gpttools} provides a few pre-defined tasks, such as \"conservative q&a\" or \"extract key libraries and tools\", which can be passed to this argument. These queries were taken from a repo created by OpenAI and Pinecone, which can be found [here](https://github.com/pinecone-io/examples/tree/master/integrations/openai/beyond_search_webinar).\n* `k`: The number of similar chunks of text to return.\n\nThe function generates an embedding for the query string with `create_openai_embedding()`. It then uses the `get_top_matches()` to find the most similar chunks of text in the index using cosine similarity returning the top `k` matches.\n\nThe next step is to formats the instructions based on the `task` argument as well as the `query` and `context`. For example, if the task is \"conservative q&a\", the function will return a string asking the model to answer a question based on the context of the returned chunks of text. If the task is \"extract key libraries and tools\", the function will return a string listing the libraries and tools present in the returned chunks of text.\n\nThe prompt is passed to OpenAI's GPT-3.5 `gpt-3.5-turbo` (i.e., ChatGPT) to generate a response based on the formatted output. The response is returned as a list containing the instructions, the top `k` matches, and the response from ChatGPT.\n\n## Analysis of Embeddings with tidymodels and UMAP\n\nThe next step is to perform dimension reduction with UMAP as naive exploration of the embeddings. The `embeddings` column in the index contains a list of 1536-dimensional vectors. We can use the `recipes` package to normalize the vectors and then use the `step_umap()` function to reduce the dimensionality to 2. We can then plot the results.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(recipes)\nlibrary(ggplot2)\nlibrary(purrr)\nlibrary(tidyr)\nlibrary(embed)\n\nindex_wide <-  \n  index |>\n  mutate(embedding = map(embedding, \\(x) as.data.frame(t(x)))) |>\n  unnest(embedding)\n\nset.seed(123)\n\numap_spec <- recipe(~ ., data = index_wide) |>\n  step_normalize(starts_with(\"V\")) |>\n  step_umap(starts_with(\"V\"), num_comp = 2)\n\numap_estimates <- prep(umap_spec, training = index_wide)\numap_data <- bake(umap_estimates, new_data = NULL)\n\numap_data |> \n  ggplot() +\n  geom_point(aes(x = UMAP1, y = UMAP2, color = link),\n             alpha = 0.5, size = 2, show.legend = FALSE) +\n  labs(title = \"Dimensionality Reduction with UMAP\",\n       subtitle = \"UMAP of 1536-dimensional vectors | Colored by Source Text\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](2023-03-10_vectorstores_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n## Retreiver Plugin as a Shiny App\n\nTo use the index, `{gpttools}` now as a shiny app that you can run as a plugin in RStudio. To use it, open the command palette (Cmd/Ctrl + Shift + P) and type \"gpttools\". You can then select the \"gpttools: ChatGPT with Retrieval\" option. This will open a shiny app in your viewer pane. If you have multiple indices, you can select which one to use in the dropdown menu. You can also specify if you want answers that only use the context of \nindex (\"Context Only\") or if you want answers that use the context of the index and full ChatGPT model (\"Permissive Chat\"). \"Context Only\" answers are much less likely to \"hallucinate.\" \"Permissive Chat\" answers are more likely to hallucinate, but they are also more likely choice if the index lacks relevant information.\n\n<video src=\"https://user-images.githubusercontent.com/6314313/227738408-0c4c97e9-3601-4977-b8a8-ac655a185656.mov\" data-canonical-src=\"https://user-images.githubusercontent.com/6314313/227738408-0c4c97e9-3601-4977-b8a8-ac655a185656.mov\" controls=\"controls\" muted=\"muted\" class=\"d-block rounded-bottom-2 width-fit\" style=\"max-height:400px; max-width: 100%; width: 100%;\">\n</video>\n\n## Photo Credit\n\nThumbnail Photo by <a href=\"https://unsplash.com/@richardworks?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Richard Burlton</a> on <a href=\"https://unsplash.com/s/photos/retriever?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>",
    "supporting": [
      "2023-03-10_vectorstores_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}