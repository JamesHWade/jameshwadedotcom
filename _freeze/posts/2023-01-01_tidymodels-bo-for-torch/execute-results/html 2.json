{
  "hash": "459ba3eb0c3fd50ef89bd25c7e2c93be",
  "result": {
    "markdown": "---\ntitle: Bayesian Optimizaiton with Tidymodels\nformat: html\nauthor: James H Wade\ndate: 2023-01-01\ndescription: Model tuning or torch models with Bayesian optimization using tune, workflows, brulee, and other friends from tidymodels\nimage: images/brulee.jpg\ncategories: \n  - machine learning\n  - modeling\n  - tune\n  - deep learning\n  - torch\n  - R\nexecute: \n  freeze: auto\ncode-copy: true\n---\n\n\nHyperparameter optimization is a key part of the machine learning workflow. Knowing what hyperparameters to choose or even which ones to change can be a bit overwhelming, especially when you have a lot of them. Iterative hyperparameter optimization is a common approach to this problem, but it can be time consuming and expensive. Bayesian optimization is a method that can help with this problem. For a deeper dive into Bayesian optimization and iterative optimization overall, Julia Silge and Max Kuhn's [*Tidy Modeling with R*](https://www.tmwr.org) has a [great chapter on this topic](https://www.tmwr.org/iterative-search.html).\n\nIn [the whole game](2022-12-27_mlops-the-whole-game.qmd), I used Bayesian optimization for hyperparameterization but did not provide much explanation or justification. In this post, we'll use Bayesian optimization to tune the hyperparameters of a neural net with `{torch}` and `{brulee}` using the tidymodels framework.\n\nSilge and Kuhn's share some advice with how to approach hyperparameter optimization and model screening in general:\n\n> A good strategy is to spend some initial effort trying a variety of modeling approaches, determine what works best, then invest additional time tweaking/optimizing a small set of models.\n> <cite> Julia Silge and Max Kuhn, [*Tidy Modeling with R*](https://www.tmwr.org) </cite>\n\n\n::: {.cell filename='Load Packages & Set Preferences'}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(brulee)\nlibrary(modeldata)\nlibrary(tidyverse)\nlibrary(skimr)\ntidymodels_prefer()\ntheme_set(theme_minimal())\nset.seed(1234)\n```\n:::\n\n\n## The Data\n\nThe `{modeldata}` package has a number of datasets that are useful for modeling. We'll use the `ad_data` dataset, which is a clinical study of a few hundred patients with cognitive impairment. The goal of the study was to predict if early stages of cognitive impairment could be distinguished from normal cognition. We can look at the data documentation with `?modeldata::ad_data`.\n\n::: {.callout-note collapse=\"true\"}\n## Expand To Learn About `ad-data`\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show Code to Print Help Document\"}\n#' Capture help documents contents\n#'\n#' Allows you to capture the contents of a help file to print to the console or\n#' include in a Quarto / RMarkdown document.\n#'\n#' based on code by Noam Ross\n#'  http://www.noamross.net/archives/2013-06-18-helpconsoleexample/\n#' Stéphane Laurent\n#'  https://stackoverflow.com/questions/60468080/\n#'   print-an-r-help-file-vignette-as-output-into-an-r-html-notebook\n#' Michael Sumner (mdsumner)\n#'  https://stackoverflow.com/questions/7495685/\n#'   how-to-access-the-help-documentation-rd-source-files-in-r\n#' and David Fong\n#'  https://stackoverflow.com/questions/60468080/print-an-r-help-file-vignette-\n#'  as-output-into-an-r-html-notebook/62241456#62241456\n#'\n#' @param topic - the command for which help is required\n#' @param package - the package name with the required topic\n#' @param format - output format\n#' @param before - place code before the output e.g. \"<blockquote>\"\n#' @param after - place code after the output e.g. \"</blockquote>\"\nhelp_console <- function(topic, package,\n                         format = c(\"text\", \"html\", \"latex\", \"Rd\"),\n                         before = NULL, after = NULL) {\n  format <- match.arg(format)\n  if (!is.character(topic)) topic <- deparse(substitute(topic))\n  db <- tools::Rd_db(package)\n  helpfile <- db[paste0(topic, \".Rd\")][[1]]\n  hs <- capture.output(\n    switch(format,\n      text = tools::Rd2txt(helpfile,\n        stages = \"render\",\n        outputEncoding = \"UTF-8\"\n      ),\n      html = tools::Rd2HTML(helpfile, package = \"\", stages = \"render\"),\n      latex = tools::Rd2latex(helpfile),\n      Rd = tools:::prepare_Rd(helpfile)\n    )\n  )\n  if (format == \"html\") {\n    i <- grep(\"<body>\", hs)\n    j <- grep(\"</body>\", hs)\n    hs <- hs[(i + 1):(j - 1)]\n  }\n  hs <- c(before, hs, after)\n  hs <- cat(hs, sep = \"\\n\")\n  invisible(hs)\n}\nhelp_console(\"ad_data\", format = \"text\", package = \"modeldata\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n_\bA_\bl_\bz_\bh_\be_\bi_\bm_\be_\br'_\bs _\bd_\bi_\bs_\be_\ba_\bs_\be _\bd_\ba_\bt_\ba\n\n_\bD_\be_\bs_\bc_\br_\bi_\bp_\bt_\bi_\bo_\bn:\n\n     Alzheimer's disease data\n\n_\bD_\be_\bt_\ba_\bi_\bl_\bs:\n\n     Craig-Schapiro et al. (2011) describe a clinical study of 333\n     patients, including some with mild (but well-characterized)\n     cognitive impairment as well as healthy individuals. CSF samples\n     were taken from all subjects. The goal of the study was to\n     determine if subjects in the early states of impairment could be\n     differentiated from cognitively healthy individuals. Data\n     collected on each subject included:\n\n        • Demographic characteristics such as age and gender\n\n        • Apolipoprotein E genotype\n\n        • Protein measurements of Abeta, Tau, and a phosphorylated\n          version of Tau (called pTau)\n\n        • Proteinmeasurements of 124 exploratory biomarkers, and\n\n        • Clinical dementia scores\n\n     For these analyses, we have converted the scores to two classes:\n     impaired and healthy. The goal of this analysis is to create\n     classification models using the demographic and assay data to\n     predict which patients have early stages of disease.\n\n_\bV_\ba_\bl_\bu_\be:\n\n ad_data: a tibble\n\n_\bS_\bo_\bu_\br_\bc_\be:\n\n     Kuhn, M., Johnson, K. (2013) _Applied Predictive Modeling_,\n     Springer.\n\n     Craig-Schapiro R, Kuhn M, Xiong C, Pickering EH, Liu J, Misko TP,\n     et al. (2011) Multiplexed Immunoassay Panel Identifies Novel CSF\n     Biomarkers for Alzheimer's Disease Diagnosis and Prognosis. PLoS\n     ONE 6(4): e18850.\n\n_\bE_\bx_\ba_\bm_\bp_\bl_\be_\bs:\n\n     data(ad_data)\n     str(ad_data)\n     \n```\n:::\n:::\n\n\nSummarizing data with `{skimr}` can give a quick feel for the data overall. Remove ` |> summary()` from the code chunk below for an even more descriptive output. I did not include it here because there are so many variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nskim(ad_data) |> summary()\n```\n\n::: {.cell-output-display}\nTable: Data summary\n\n|                         |        |\n|:------------------------|:-------|\n|Name                     |ad_data |\n|Number of rows           |333     |\n|Number of columns        |131     |\n|_______________________  |        |\n|Column type frequency:   |        |\n|factor                   |2       |\n|numeric                  |129     |\n|________________________ |        |\n|Group variables          |None    |\n:::\n:::\n\n\n:::\n\n### Data Splitting\n\nWe'll use a deep neural net with to predict the `Class` variable. But first, we want to split the data into a training and testing set. We'll use the `initial_split()` function from the `{rsample}` package to do this. The default training and testing split is 75% training and 25% testing, which is a good place to start. For the sampling, we'll use the `Class` variable as the strata. This will ensure that the training and testing sets have the same proportion of each class.\n\n\n::: {.cell filename='Initial Split'}\n\n```{.r .cell-code}\nad_split <- initial_split(ad_data, strata = Class)\nad_train <- training(ad_split)\nad_test  <- testing(ad_split)\n```\n:::\n\n\n### Cross Validation\n\nWe'll use v-fold cross validation to evaluate the model. We'll use the `vfold_cv()` function from the `{rsample}` package to do this. We'll use the `Class` variable as the strata again to ensure that each fold has the same proportion of each class.\n\n\n::: {.cell filename='Cross Validation'}\n\n```{.r .cell-code}\nad_folds <- vfold_cv(ad_train, v = 3, strata = Class)\n```\n:::\n\n\n## Build the Model\n\n### Data Preprocessing with `{recipes}`\n\nWe'll use the `{recipes}` package to preprocess the data include a few standard \npre-processing steps following the advice from \n[`recipes` documentation for order of steps](https://recipes.tidymodels.org/\narticles/Ordering.html#recommended-preprocessing-outline)]:\n-  `step_YeoJohnson()` to transform the numeric variables\n-  `step_dummy()` to create dummy variables for the categorical variables\n-  `step_normalize()` to normalize the numeric variables\n-  `step_nzv()` to remove near-zero variance variables\n\n\n::: {.cell filename='Specify Recipe'}\n\n```{.r .cell-code}\nad_rec <-\n  recipe(Class ~ ., data = ad_train) |>\n  step_YeoJohnson(all_numeric_predictors()) |>\n  step_normalize(all_numeric_predictors()) |>\n  step_dummy(all_nominal_predictors()) |>\n  step_nzv(all_numeric_predictors())\n```\n:::\n\n\n### Model Specification\n\nWe will use two models to demonstrate hyperparameter tuning: logistic regression and multilayer perception. Model specification is beyond the scope of this post, but you can read more about it in the [tidymodels documentation](https://www.tidymodels.org/learn/) or in [Tidy Models with R](https://www.tmwr.org/). For now, we'll just specify the models.\n\n\n::: {.cell filename='Specify Models'}\n\n```{.r .cell-code}\nlogistic_reg_brulee_spec <-\n  logistic_reg(\n    penalty = tune()\n  ) |>\n  set_engine(\"brulee\")\n\nmlp_brulee_spec <-\n  mlp(\n    hidden_units = 10,\n    dropout      = tune(),\n    epochs       = tune(),\n    learn_rate   = tune(),\n    activation   = \"elu\"\n  ) |>\n  set_engine(\"brulee\") |>\n  set_mode(\"classification\")\n```\n:::\n\n\n### Model Tuning\n\nModel tuning is where Bayesian optimization comes into play. The `{tune}` package is quite handy for this. In particular, `tune::tune_bayes()` and `tune::control_bayes()` are the functions we'll use. The `tune_bayes()` function takes a model specification, the recipe, the data, and the cross validation folds. The `control_bayes()` function takes a few parameters that control the Bayesian optimization:\n\n- `no_improve` controls how many iterations of Bayesian optimization to run without improvement\n- `time_limit` controls how long to run Bayesian optimization in minutes. \n- `save_pred` controls whether to save the predictions from each iteration of Bayesian optimization. This is useful for ensembling.\n- `save_workflow` controls whether to save the workflow should be appended to the results.\n- `verbose` and `verbose_iter` controls whether to print the results of each iteration of Bayesian optimization.\n- `allow_par` and `parallel_over` controls whether to run tuning in parallel. This only works for some engines, and I don't think it works for `brulee` or `keras` yet.\n\n\n\n::: {.cell filename='Tuning Control Settings'}\n\n```{.r .cell-code}\nbayes_control <-\n  control_bayes(\n    no_improve    = 30L,\n    time_limit    = 20,\n    verbose_iter  = TRUE,\n    save_pred     = TRUE,\n    save_workflow = TRUE\n  )\n\ngrid_control <-\n  control_grid(\n    allow_par     = TRUE,\n    save_pred     = TRUE,\n    save_workflow = TRUE,\n    parallel_over = NULL\n  )\n```\n:::\n\n\nThe basic intuition behind Bayesian optimization is that it uses a surrogate model to approximate the true model. The surrogate model is a probabilistic model that is updated with each iteration of Bayesian optimization and is used to find the next set of hyperparameters to evaluate. This process is repeated until the surrogate model is no longer improving or the time limit is reached. For `tune_bayes()`, the surrogate model is a Gaussian process model. \n\nIt's a good idea to adjust the range of hyperparameter values before we start to fit our model, and the {dials} package can help. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndials::penalty()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAmount of Regularization (quantitative)\nTransformer: log-10 [1e-100, Inf]\nRange (transformed scale): [-10, 0]\n```\n:::\n\n```{.r .cell-code}\ndials::activation()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nActivation Function  (qualitative)\n5 possible value include:\n'linear', 'softmax', 'relu', 'elu' and 'tanh' \n```\n:::\n\n```{.r .cell-code}\ndials::epochs()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Epochs (quantitative)\nRange: [10, 1000]\n```\n:::\n\n```{.r .cell-code}\ndials::dropout()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDropout Rate (quantitative)\nRange: [0, 1)\n```\n:::\n\n```{.r .cell-code}\ndials::hidden_units()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Hidden Units (quantitative)\nRange: [1, 10]\n```\n:::\n:::\n\n\nThe default range for `epochs` is a bit large, but we can update it. Let's also narrow the range for dropout from `c(0, 1)` to `(0.1, 0.9)`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmlp_brulee_params <-\n  mlp_brulee_spec |>\n  extract_parameter_set_dials() |>\n  update(\n    epochs  = epochs(c(10, 200)),\n    dropout = dropout(c(0.1, 0.9))\n  )\n```\n:::\n\n\nWe can also use the `grid_regular()` function to create a grid of hyperparameter values to evaluate. We'll use this to create a grid of hyperparameter values to serve as a starting point for Bayesian optimization.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmlp_brulee_start <-\n  mlp_brulee_params |>\n  grid_regular(levels = 3)\n```\n:::\n\n\n## Model Workflow\n\nThe `{workflows}` package creates a workflow for each model. The workflow will include the recipe, the model specification, and the cross validation folds. We'll use the `workflow()` function to create the workflow. The `tune_bayes()` function will then be used to tune the model with with splits and control parameters we created above.\n\n### Logistic Regression\n\nWe start with a logistic regression model. We only have one hyperparameter to tune, so we'll use the `tune_grid()` function instead of `tune_bayes()`.\n\n\n::: {.cell filename='Create Logistic Regression Workflow'}\n\n```{.r .cell-code}\nlogistic_reg_brulee_wf <-\n  workflow() |>\n  add_recipe(ad_rec) |>\n  add_model(logistic_reg_brulee_spec) |>\n  tune_grid(\n    resamples = ad_folds,\n    control   = grid_control\n  )\n```\n:::\n\n\nWe can use `autoplot()` to visualize the results of tuning.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(logistic_reg_brulee_wf)\n```\n\n::: {.cell-output-display}\n![](2023-01-01_tidymodels-bo-for-torch_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nWe can also use `collect_metrics()` to collect the results of tuning.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(logistic_reg_brulee_wf, summarize = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 20 × 7\n    penalty .metric  .estimator  mean     n std_err .config              \n      <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n 1 1.11e- 2 accuracy binary     0.831     3 0.0257  Preprocessor1_Model01\n 2 1.11e- 2 roc_auc  binary     0.880     3 0.00871 Preprocessor1_Model01\n 3 4.58e- 8 accuracy binary     0.835     3 0.00419 Preprocessor1_Model02\n 4 4.58e- 8 roc_auc  binary     0.878     3 0.00766 Preprocessor1_Model02\n 5 7.60e- 9 accuracy binary     0.843     3 0.0121  Preprocessor1_Model03\n 6 7.60e- 9 roc_auc  binary     0.874     3 0.00235 Preprocessor1_Model03\n 7 3.85e- 3 accuracy binary     0.855     3 0.0183  Preprocessor1_Model04\n 8 3.85e- 3 roc_auc  binary     0.895     3 0.00220 Preprocessor1_Model04\n 9 1.76e- 5 accuracy binary     0.839     3 0.0115  Preprocessor1_Model05\n10 1.76e- 5 roc_auc  binary     0.891     3 0.00872 Preprocessor1_Model05\n11 6.88e- 7 accuracy binary     0.827     3 0.0187  Preprocessor1_Model06\n12 6.88e- 7 roc_auc  binary     0.877     3 0.0192  Preprocessor1_Model06\n13 3.57e- 6 accuracy binary     0.868     3 0.0182  Preprocessor1_Model07\n14 3.57e- 6 roc_auc  binary     0.877     3 0.00511 Preprocessor1_Model07\n15 2.11e-10 accuracy binary     0.831     3 0.0130  Preprocessor1_Model08\n16 2.11e-10 roc_auc  binary     0.880     3 0.00217 Preprocessor1_Model08\n17 8.03e- 1 accuracy binary     0.827     3 0.00706 Preprocessor1_Model09\n18 8.03e- 1 roc_auc  binary     0.877     3 0.0115  Preprocessor1_Model09\n19 9.42e- 4 accuracy binary     0.823     3 0.0250  Preprocessor1_Model10\n20 9.42e- 4 roc_auc  binary     0.882     3 0.00516 Preprocessor1_Model10\n```\n:::\n:::\n\n\nFrom the fitted workflow we can select the best model with the `tune::select_best()` function and the `roc_auc` metric. This metric is used to measure the ability of the model to distinguish between two classes, and is calculated by plotting the true positive rate against the false positive rate.\n\nOnce we've identified the best model, we can extract it from the workflow using the `extract_workflow` function. This function allows us to isolate the model and use it for further analysis. We then use the `finalize_workflow` function to finalize the model, and the `last_fit` function to fit the model to the ad_split data.\n\nWe use the `collect_metrics` function to gather metrics for the best model. This is an important step, as it allows us to evaluate the performance of the model and determine whether it is accurate and reliable.\n\nFinally, we use the `collect_predictions` function to generate predictions on the test set, and use these predictions to create an ROC curve.\n\n\n\n::: {.cell filename='Select Best Logistic Regression Model and Evaluate'}\n\n```{.r .cell-code}\n# select the best model from the workflow\nbest_logistic_reg_id <-\n  logistic_reg_brulee_wf |>\n  select_best(metric = \"roc_auc\")\n\n# extract the best model from the workflow\nbest_logistic_reg <-\n  logistic_reg_brulee_wf |>\n  extract_workflow() |>\n  finalize_workflow(best_logistic_reg_id) |>\n  last_fit(ad_split)\n\n# collect the metrics for the best model\nbest_logistic_reg |>\n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.833 Preprocessor1_Model1\n2 roc_auc  binary         0.865 Preprocessor1_Model1\n```\n:::\n\n```{.r .cell-code}\n# plot results of test set fit\nbest_logistic_reg |>\n  collect_predictions() |>\n  roc_curve(Class, .pred_Impaired) |>\n  autoplot()\n```\n\n::: {.cell-output-display}\n![](2023-01-01_tidymodels-bo-for-torch_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n### Multilayer Perceptron\n\nWe start by fitting the workflow with the grid of hyperparameter values we created above. This will give us a starting point for Bayesian optimization. \n\n\n::: {.cell filename='Create MLP Workflow and Perform Grid Tuning'}\n\n```{.r .cell-code}\nmlp_brulee_wf <-\n  workflow() |>\n  add_recipe(ad_rec) |>\n  add_model(mlp_brulee_spec)\n\nmlp_brulee_tune_grid <-\n  mlp_brulee_wf |>\n  tune_grid(\n    resamples = ad_folds,\n    grid      = mlp_brulee_start,\n    control   = grid_control\n  )\n```\n:::\n\n\nAs above, `autoplot()` is a quick way to visualize results form our initial grid tuning.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(mlp_brulee_tune_grid)\n```\n\n::: {.cell-output-display}\n![](2023-01-01_tidymodels-bo-for-torch_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\nWe can also repeat the best model selection and evaluation, as we did for logistic regression. Our expectation should be that Bayesian optimization results in better predictions that a simple `tune_grid()` approach.\n\n\n::: {.cell filename='Visualize and Evaluate Initial Tuning'}\n\n```{.r .cell-code}\nbest_mlp_id_no_bayes <-\n  mlp_brulee_tune_grid |>\n  select_best(metric = \"roc_auc\")\n\n# extract the best model from the workflow\nbest_mlp_no_bayes <-\n  mlp_brulee_tune_grid |>\n  extract_workflow() |>\n  finalize_workflow(best_mlp_id_no_bayes) |>\n  last_fit(ad_split)\n\n# collect the metrics for the best model\nbest_mlp_no_bayes |>\n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.833 Preprocessor1_Model1\n2 roc_auc  binary         0.852 Preprocessor1_Model1\n```\n:::\n\n```{.r .cell-code}\n# plot results of test set fit\nbest_mlp_no_bayes |>\n  collect_predictions() |>\n  roc_curve(Class, .pred_Impaired) |>\n  autoplot()\n```\n\n::: {.cell-output-display}\n![](2023-01-01_tidymodels-bo-for-torch_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\nWe now take the initial MLP tune results and pass them to `tune_bayes()`, which includes the initial grid of hyperparameter values, the folds we created above, and the `bayes_control` variables we created earlier.\n\n\n::: {.cell filename='Bayesian Optimization of MLP'}\n\n```{.r .cell-code}\nmlp_brulee_bo <-\n  mlp_brulee_wf |>\n  tune_bayes(\n    resamples = ad_folds,\n    iter      = 100L,\n    control   = bayes_control,\n    initial   = mlp_brulee_tune_grid\n  )\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nOptimizing roc_auc using the expected improvement\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! No improvement for 30 iterations; returning current results.\n```\n:::\n:::\n\n\nYet again, we can use `autoplot()` to visualize the results of tuning. From this, it appears that learning rate has a big impact on model performance, but number of epochs and droput rate are less important. Digging into why is beyond the scope of this post, but it's important to recognize that not all hyperparameter are created equal. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(mlp_brulee_bo)\n```\n\n::: {.cell-output-display}\n![](2023-01-01_tidymodels-bo-for-torch_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\nAnd yet again, we can use `collect_metrics()` to collect the results of tuning.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(mlp_brulee_bo, summarize = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 142 × 10\n   dropout epochs learn_rate .metric  .estim…¹  mean     n std_err .config .iter\n     <dbl>  <int>      <dbl> <chr>    <chr>    <dbl> <int>   <dbl> <chr>   <int>\n 1     0.1     10      0.001 accuracy binary   0.426     3  0.0897 Prepro…     0\n 2     0.1     10      0.001 roc_auc  binary   0.491     3  0.0212 Prepro…     0\n 3     0.5     10      0.001 accuracy binary   0.563     3  0.111  Prepro…     0\n 4     0.5     10      0.001 roc_auc  binary   0.542     3  0.0360 Prepro…     0\n 5     0.9     10      0.001 accuracy binary   0.466     3  0.0362 Prepro…     0\n 6     0.9     10      0.001 roc_auc  binary   0.554     3  0.0576 Prepro…     0\n 7     0.1    105      0.001 accuracy binary   0.357     3  0.0274 Prepro…     0\n 8     0.1    105      0.001 roc_auc  binary   0.459     3  0.0476 Prepro…     0\n 9     0.5    105      0.001 accuracy binary   0.610     3  0.0619 Prepro…     0\n10     0.5    105      0.001 roc_auc  binary   0.550     3  0.0707 Prepro…     0\n# … with 132 more rows, and abbreviated variable name ¹​.estimator\n```\n:::\n:::\n\n\nFor our moment of truth, we can select the best model and evaluate it on the test set.\n\n\n::: {.cell filename='Finalize Model and Evaluate'}\n\n```{.r .cell-code}\nbest_mlp_id <-\n  mlp_brulee_bo |>\n  select_best(metric = \"roc_auc\")\n\n# extract the best model from the workflow\nbest_mlp <-\n  mlp_brulee_bo |>\n  extract_workflow() |>\n  finalize_workflow(best_mlp_id) |>\n  last_fit(ad_split)\n\n# collect the metrics for the best model\nbest_mlp |>\n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.893 Preprocessor1_Model1\n2 roc_auc  binary         0.890 Preprocessor1_Model1\n```\n:::\n\n```{.r .cell-code}\n# plot results of test set fit\nbest_mlp |>\n  collect_predictions() |>\n  roc_curve(Class, .pred_Impaired) |>\n  autoplot()\n```\n\n::: {.cell-output-display}\n![](2023-01-01_tidymodels-bo-for-torch_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\nBased on these results, there was not much value in the Bayesian optimization in this case. Nonetheless, it is a useful tool to have in your toolbox, and I hope you find this example useful.\n",
    "supporting": [
      "2023-01-01_tidymodels-bo-for-torch_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}