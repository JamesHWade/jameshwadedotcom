{
  "hash": "cecb316e56c458a3d92fa33a0fba4056",
  "result": {
    "markdown": "---\ntitle: \"MLOps: Moving from Posit Connect to Azure\"\nauthor: James H Wade\ndate: 2023-01-22\nexecute:\n  freeze: auto\nformat:\n  html:\n    toc: true\n    code-copy: true\nparams:\n  AZURE_SAS_KEY=`Sys.getenv(\"AZURE_SAS_KEY\")`\n  AZURE_CONTAINER_ENDPOINT=`Sys.getenv(\"AZURE_CONTAINTER_ENDPOINT\")`\n---\n\n\nIf you're like me, the decisions about deployment locations and \"the cloud\" are out of your control at work. Whether you use AWS, GCP, Azure, or another, you are stuck with the cloud you've been given. The purpose of this article is to demonstrate a model deployment using Posit's open source tools for MLOps and using Azure as the deployment infrastructure. This is the second article in a series on MLOps. See the first one that [uses Posit Connect for deployment](2022-12-27_mlops-the-whole-game.qmd).\n\n[![Source: MLOps Team at Posit \\| An overview of MLOps with Vetiver and friends](images/vetiver-mlops.png){fig-alt=\"During the MLOps cycle, we collect data, understand and clean the data, train and evaluate a model, deploy the model, and monitor the deployed model. Monitoring can then lead back to collecting more data. There are many great tools available to understand clean data (like pandas and the tidyverse) and to build models (like tidymodels and scikit-learn). Use the vetiver framework to deploy and monitor your models.\"}](https://vetiver.rstudio.com/)\n\n## Model Building\n\nWe covered model builidng in [part one](2022-12-27_mlops-the-whole-game.qmd), but here is the code from there to save time searching around for it.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show code from part one\"}\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(tidymodels)\nlibrary(pins)\nlibrary(vetiver)\nlibrary(palmerpenguins)\nlibrary(plumber)\nlibrary(conflicted)\ntidymodels_prefer()\nconflict_prefer(\"penguins\", \"palmerpenguins\")\npenguins_df <-\n  penguins |>\n  drop_na(sex) |>\n  select(-year, -island)\n\nset.seed(1234)\npenguin_split <- initial_split(penguins_df, strata = sex)\npenguin_train <- training(penguin_split)\npenguin_test <- testing(penguin_split)\n\npenguin_rec <-\n  recipe(sex ~ ., data = penguin_train) |>\n  step_YeoJohnson(all_numeric_predictors()) |>\n  step_normalize(all_numeric_predictors()) |>\n  step_dummy(species)\n\nglm_spec <-\n  logistic_reg() |>\n  set_engine(\"glm\")\n\ntree_spec <-\n  rand_forest(min_n = tune()) |>\n  set_engine(\"ranger\") |>\n  set_mode(\"classification\")\n\nmlp_brulee_spec <-\n  mlp(\n    hidden_units = tune(), epochs = tune(),\n    penalty = tune(), learn_rate = tune()\n  ) %>%\n  set_engine(\"brulee\") %>%\n  set_mode(\"classification\")\n\nset.seed(1234)\npenguin_folds <- vfold_cv(penguin_train)\n\nbayes_control <-\n  control_bayes(no_improve = 10L, time_limit = 20, save_pred = TRUE)\n\nset.seed(1234)\nworkflow_set <-\n  workflow_set(\n    preproc = list(penguin_rec),\n    models = list(\n      glm = glm_spec,\n      tree = tree_spec,\n      torch = mlp_brulee_spec\n    )\n  ) |>\n  workflow_map(\"tune_bayes\",\n    iter = 50L,\n    resamples = penguin_folds,\n    control = bayes_control\n  )\n```\n:::\n\n\n## Model Deployment on Azure\n\nThe [`{vetiver}`](https://rstudio.github.io/vetiver-r/) package is provides a set of tools for building, deploying, and managing machine learning models in production. It allows users to easily create, version, and deploy machine learning models to various hosting platforms, such as Posit Connect or a cloud hosting service like Azure. Part one showed a Connect deployment, and this one will use an Azure storage container as the board.\n\nThe `vetiver_model()` function is used to create an object that stores a machine learning model and its associated metadata, such as the model's name, type, and parameters. `vetiver_pin_write()` and `vetiver_pin_read()` functions are used to write and read `vetiver_model` objects to and from a server.\n\n### Create an Pins Board in an Azure Storage Container\n\nTo access an Azure storage container, we can use the [`{AzureStor}`](https://github.com/Azure/AzureStor) packages. If you are using Azure, you are most likely using it in a corporate environment. That often comes with company-specific policies. If these are new to you, your best bet is to find someone who is already familiar with the cloud environment at your organization. This example uses SAS (Shared Access Signature) key authentication, which is a way to grant limited access to Azure storage resources, such as containers, to users or applications. SAS keys are generated by Azure Storage and provide a secure way to access storage resources without sharing the account key or the access keys associated with the storage account.\n\nTo use SAS keys for accessing Azure storage containers, you will need to create a SAS key and use it to authenticate your requests to the storage API. You can learn more about SAS keys and how to generate them from [Microsoft Learn](https://learn.microsoft.com/en-us/azure/applied-ai-services/form-recognizer/create-sas-tokens?).\n\nBelow is an example for how to access an Azure storage container, create or connect to a board, and list pins stored in side it if any exist. The code assumes that the user has already set the AZURE_CONTAINER_ENDPOINT and AZURE_SAS_KEY environment variables and has installed the AzureStor and pins packages in their R environment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(AzureStor)\nlibrary(pins)\n\ncontainer <-\n  storage_container(\n    endpoint = Sys.getenv(\"AZURE_CONTAINER_ENDPOINT\"),\n    sas = Sys.getenv(\"AZURE_SAS_KEY\")\n  )\n\nmodel_board <- pins::board_azure(container)\n```\n:::\n\n\n\n\nThe `storage_container()` function from the AzureStor package is used to create a storage container object, which represents a container in an Azure storage account. The endpoint parameter specifies the endpoint URL for the storage container, and the `sas` variable specifies a SAS key that is used to authenticate requests to the container.\n\nThe `Sys.getenv()` function is used to retrieve the values of the `AZURE_CONTAINER_ENDPOINT` and `AZURE_SAS_KEY` environment variables. This assumes you already set `AZURE_CONTAINER_ENDPOINT` and `AZURE_SAS_KEY` in something like a `.Renviron` file. These variables should contain the endpoint URL and SAS key for the Azure storage container, respectively.\n\nThe `board_azure()` function from the `{pins}` package creates a pins board object that in the Azure storage container.\n\n### Create Vetiver Model\n\nTo deploy our model with `{vetiver}`, we starting with our `final_fit_to_deploy` from above, we first need to extract the trained workflow.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n\nbest_model_id <- \"recipe_glm\"\n\nbest_fit <-\n  workflow_set |>\n  extract_workflow_set_result(best_model_id) |>\n  select_best(metric = \"accuracy\")\n\nfinal_fit_to_deploy <-\n  workflow_set |>\n  extract_workflow(best_model_id) |>\n  finalize_workflow(best_fit) |>\n  last_fit(penguin_split) |>\n  extract_workflow()\n```\n:::\n\n\nWe can do that with `tune::extract_workflow()`. The trained workflow is what we will deploy as a `vetiver_model`. That means we need to convert it from a workflow to a vetiver model with `vetiver_model()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(vetiver)\nv <- vetiver_model(final_fit_to_deploy, model_name = \"penguins_model\")\n\nv\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n── penguins_model ─ <bundled_workflow> model for deployment \nA glm classification modeling workflow using 5 features\n```\n:::\n:::\n\n\n### Pin Model to Board\n\nOnce the model_board connection is made it's as easy as `vetiver_pin_write()` to \"pin\" our model to the model board and `vetiver_pin_read()` to access it. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_board |> vetiver_pin_write(v)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nCreating new version '20230122T144248Z-a875f'\nWriting to pin 'penguins_model'\n\nCreate a Model Card for your published model\n• Model Cards provide a framework for transparent, responsible reporting\n• Use the vetiver `.Rmd` template as a place to start\n```\n:::\n\n```{.r .cell-code}\nmodel_board |> vetiver_pin_read(\"penguins_model\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n── penguins_model ─ <bundled_workflow> model for deployment \nA glm classification modeling workflow using 5 features\n```\n:::\n:::\n\n\n### Create Model API\n\nOur next step is to use `{vetiver}` and [`{plumber}`](https://www.rplumber.io/) packages to create an API for our vetiver model, which can then be accessed and used to make predictions or perform other tasks via an HTTP request. `pr()` creates a new plumber router, and `vetiver_api(v)` adds a `POST` endpoint to make endpoints from a trained vetiver model. `vetiver_write_plumber()` creates a `plumber.R` file that specifies the model version of the model we pinned to our model dashboard with `vetiver_pin_write()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(plumber)\npr() |>\n  vetiver_api(v)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Plumber router with 2 endpoints, 4 filters, and 1 sub-router.\n# Use `pr_run()` on this object to start the API.\n├──[queryString]\n├──[body]\n├──[cookieParser]\n├──[sharedSecret]\n├──/logo\n│  │ # Plumber static router serving from directory: /Library/Frameworks/R.framework/Versions/4.2/Resources/library/vetiver\n├──/ping (GET)\n└──/predict (POST)\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nvetiver_write_plumber(\n  board = model_board,\n  name = \"penguins_model\",\n  file = \"azure_plumber.R\"\n)\n```\n:::\n\n\nHere is an example of the `azure_plumber.R` file generated by `vetiver_write_pumber()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generated by the vetiver package; edit with care\n\nlibrary(pins)\nlibrary(plumber)\nlibrary(rapidoc)\nlibrary(vetiver)\n\n# Packages needed to generate model predictions\nif (FALSE) {\n  library(parsnip)\n  library(recipes)\n  library(stats)\n  library(workflows)\n}\nb <- board_azure(AzureStor::storage_container(\"https://penguinstore.blob.core.windows.net/penguincontainer\"), path = \"\")\nv <- vetiver_pin_read(b, \"penguins_model\", version = \"20221222T172651Z-50d8c\")\n\n#* @plumber\nfunction(pr) {\n  pr %>% vetiver_api(v)\n}\n```\n:::\n\n\n### Deploying Elsewhere with Docker\n\nIf Posit Connect is not the right place for our model, `vetiver_write_docker` creates a `dockerfile` and `renv.lock`. Deployment is much more complicated when not using Posit Connect. If this is your first time creating a deployment, I recommend you connect with [me](mailto:jhwade@dow.com?subject=Request%20for%20Help%20with%20Vetiver%20Deployment&body=Hi,%20I%20was%20reading%20your%20post%20on%20deploying%20outside%20of%20Posit%20Connect.%20I'd%20like%20some%20help.%20For%20my%20project...) or someone else with experience in Azure deployments.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvetiver_write_docker(\n  vetiver_model = v,\n  path = \"azure\",\n  lockfile = \"azure/vetiver_renv.lock\"\n)\n```\n:::\n\n\nHere is an example of the dockerfile that is generated.\n\n``` dockerfile\n# Generated by the vetiver package; edit with care\n\nFROM rocker/r-ver:4.2.2\nENV RENV_CONFIG_REPOS_OVERRIDE packagemanager.rstudio.com/cran/latest\n\nRUN apt-get update -qq && apt-get install -y --no-install-recommends \\\n  libcurl4-openssl-dev \\\n  libicu-dev \\\n  libsodium-dev \\\n  libssl-dev \\\n  make \\\n  zlib1g-dev \\\n  && apt-get clean\n\nCOPY azure/vetiver_renv.lock renv.lock\nRUN Rscript -e \"install.packages('renv')\"\nRUN Rscript -e \"renv::restore()\"\nCOPY plumber.R /opt/ml/plumber.R\nEXPOSE 8000\nENTRYPOINT [\"R\", \"-e\", \"pr <- plumber::plumb('/opt/ml/plumber.R'); pr$run(host = '0.0.0.0', port = 8000)\"]\n```\n\nTo deploy our API in Azure using that Dockerfile, we need to:\n\n1.  Build a Docker image of your API using the Dockerfile. We need to have \\[docker installed\\](https://docs.docker.com/get-docker/) on the system we use to build the container. You can build the docker image from the Dockerfile by running the following command in the directory where your Dockerfile is located:\n\n\n::: {.cell filename='Terminal'}\n\n```{.bash .cell-code}\ndocker build -t penguin-image .\n```\n:::\n\n\n2.  Push the Docker image to a container registry. A container registry is a service that stores Docker images and makes them available for deployment. Azure's registry is called Azure Container Registry (ACR). Before we can push the image to ACR, we need to log in to the ACR using the `az acr login` command from the Azure CLI. We also need to create an ACR instance in Azure if we don't already have one. To push the Docker image to a container registry, you will need to use the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) `docker push` command and specify the image name and the registry URL. For example, to push the image to ACR, you can use the following command:\n\n\n::: {.cell filename='Terminal'}\n\n```{.bash .cell-code}\naz acr login --name vetiverdeploy\ndocker tag penguin-image:latest vetiverdeploy.azurecr.io/penguin-image\ndocker push vetiverdeploy.azurecr.io/penguin-image\n```\n:::\n\n\nHere, `vetiverdeploy` is the name of our ACR and `penguin-image` is the name of our Docker image. The `latest` tag indicates that this is the latest version of the image. For more information on how to push a Docker image to ACR, you can refer to the official Microsoft documentation: [**Push and pull Docker images with Azure Container Registry Tasks**](https://docs.microsoft.com/en-us/azure/container-registry/container-registry-tasks-quick-task). To break down these commands a bit further:\n\n-   `az acr login --name vetiverdeploy` logs in to the Azure Container Registry with the specified name (in this case, `vetiverdeploy`). This is necessary in order to push images to the registry.\n\n-   `docker tag penguin-image:latest vetiverdeploy.azurecr.io/penguin-image` tags the Docker image with the specified image name and registry URL. The image name is `penguin-image`, and the registry URL is `vetiverdeploy.azurecr.io/penguin-image`. The latest tag indicates that this is the latest version of the image.\n\n-   `docker push vetiverdeploy.azurecr.io/penguin-image` pushes the Docker image to the specified registry URL. In this case, the image will be pushed to the `vetiverdeploy` ACR.\n\n3.  We now need to create an Azure Container Instance (ACI) that uses our docker image we created and registered above. This can be done either using the [Azure CLI](https://learn.microsoft.com/en-us/azure/container-instances/container-instances-quickstart) or in the [Azure Portal](https://learn.microsoft.com/en-us/azure/container-instances/container-instances-quickstart).\n\nWith the ACI build complete, we have successfully deployed our API!\n\n::: callout-warning\n## Azure can be frustrating at first\n\nThese instructions are unlikely to be good enough to deploy a model without some familiarity with Azure. Please comment on this post or find someone with Azure experience for help.\n:::\n\n### Using the API to Make Predictions\n\nThe API deployment site url is `http://penguin.eastus.azurecontainer.io`, and the prediction endpoint is `http://penguin.eastus.azurecontainer.io:8000/predict`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nendpoint <-\n  vetiver_endpoint(\"http://penguin.eastus.azurecontainer.io:8000/predict\")\nendpoint\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n── A model API endpoint for prediction: \nhttp://penguin.eastus.azurecontainer.io:8000/predict\n```\n:::\n:::\n\n\nWe can make endpoints with the endpoint using `predict`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_data <- tibble(\n  species = \"Adelie\",\n  bill_length_mm = 40.5,\n  bill_depth_mm = 18.9,\n  flipper_length_mm = 180,\n  body_mass_g = 3950\n)\n\npredict(endpoint, new_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 1\n  .pred_class\n  <chr>      \n1 male       \n```\n:::\n:::\n\n\nYou can also use `{httr}` to call the API. In most cases, it is easier for R users to use `predict` rather than `httr::POST`. However, were this model written in another language, making predictions using `{httr}` would likely bet the best approach.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(httr)\nurl <- \"http://penguin.eastus.azurecontainer.io:8000/predict\"\njson_data <- jsonlite::toJSON(new_data)\nresponse <- POST(url, body = json_data)\nresponse\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nResponse [http://penguin.eastus.azurecontainer.io:8000/predict]\n  Date: 2023-01-22 14:42\n  Status: 200\n  Content-Type: application/json\n  Size: 24 B\n```\n:::\n\n```{.r .cell-code}\ncontent(response)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n[[1]]$.pred_class\n[1] \"male\"\n```\n:::\n:::\n\n\nAvoiding a language-specific approach altogether, you can use `curl` in a terminal to make API calls.\n\n\n::: {.cell filename='Terminal'}\n\n```{.bash .cell-code}\ncurl -X POST \"http://penguin.eastus.azurecontainer.io:8000/predict\" \\\n-H \"Accept: application/json\" \\\n-H \"Content-Type: application/json\" \\\n-d '[{\"species\":\"Adelie\",\"bill_length_mm\":0.5,\"bill_depth_mm\":0.5,\"flipper_length_mm\":0,\"body_mass_g\":0}]' \\\n```\n:::\n\n\n## Model Monitoring\n\nAfter deployment, we need to monitor model performance. The [MLOps with vetiver monitoring page](https://vetiver.rstudio.com/get-started/monitor.html) describes this well:\n\n> Machine learning can break quietly; a model can continue returning predictions without error, even if it is performing poorly. Often these quiet performance problems are discussed as types of model drift; data drift can occur when the statistical distribution of an input feature changes, or concept drift occurs when there is change in the relationship between the input features and the outcome.\n>\n> Without monitoring for degradation, this silent failure can continue undiagnosed. The vetiver framework offers functions to fluently compute, store, and plot model metrics. These functions are particularly suited to monitoring your model using multiple performance metrics over time. Effective model monitoring is not \"one size fits all\", but instead depends on choosing appropriate metrics and time aggregation for a given application.\n\nAs a baseline for model performance, we can start by using our training set to create original metrics for the model. We also simulate a `date_obs` column. In a real example, you should use the date the data was collected.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\npenguin_train_by_date <-\n  training(penguin_split) |>\n  rowwise() |>\n  mutate(date_obs = Sys.Date() - sample(4:10, 1)) |>\n  ungroup() |>\n  arrange(date_obs)\n\noriginal_metrics <-\n  augment(v, penguin_train_by_date) |>\n  vetiver_compute_metrics(\n    date_var = date_obs,\n    period = \"day\",\n    truth = \"sex\",\n    estimate = \".pred_class\"\n  )\n\nvetiver_plot_metrics(original_metrics)\n```\n\n::: {.cell-output-display}\n![](2022-12-30_mlops-azure_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nWe can pin the model performance metrics, just as we did with the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_board %>%\n  pin_write(original_metrics, \"penguin_metrics\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nGuessing `type = 'rds'`\nCreating new version '20230122T144253Z-dd69c'\nWriting to pin 'penguin_metrics'\n```\n:::\n:::\n\n\n### Performance over Time\n\nTo simulate the model going \"live\", let's use the test set to add more predictions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguin_test_by_date <-\n  testing(penguin_split) |>\n  rowwise() |>\n  mutate(date_obs = Sys.Date() - sample(1:3, 1)) |>\n  ungroup() |>\n  arrange(date_obs)\n\nv <-\n  model_board |>\n  vetiver_pin_read(\"penguins_model\")\n\nnew_metrics <-\n  augment(v, penguin_test_by_date) |>\n  vetiver_compute_metrics(\n    date_var = date_obs,\n    period = \"day\",\n    truth = \"sex\",\n    estimate = \".pred_class\"\n  )\n\nmodel_board |>\n  vetiver_pin_metrics(new_metrics, \"penguin_metrics\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nCreating new version '20230122T144257Z-4f9b0'\nWriting to pin 'penguin_metrics'\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 20 × 5\n   .index        .n .metric  .estimator .estimate\n   <date>     <int> <chr>    <chr>          <dbl>\n 1 2023-01-12    32 accuracy binary         0.844\n 2 2023-01-12    32 kap      binary         0.688\n 3 2023-01-13    45 accuracy binary         0.911\n 4 2023-01-13    45 kap      binary         0.82 \n 5 2023-01-14    29 accuracy binary         0.966\n 6 2023-01-14    29 kap      binary         0.931\n 7 2023-01-15    34 accuracy binary         0.912\n 8 2023-01-15    34 kap      binary         0.820\n 9 2023-01-16    44 accuracy binary         0.886\n10 2023-01-16    44 kap      binary         0.759\n11 2023-01-17    31 accuracy binary         0.903\n12 2023-01-17    31 kap      binary         0.807\n13 2023-01-18    34 accuracy binary         0.941\n14 2023-01-18    34 kap      binary         0.881\n15 2023-01-19    25 accuracy binary         0.92 \n16 2023-01-19    25 kap      binary         0.840\n17 2023-01-20    31 accuracy binary         0.839\n18 2023-01-20    31 kap      binary         0.686\n19 2023-01-21    28 accuracy binary         0.964\n20 2023-01-21    28 kap      binary         0.924\n```\n:::\n:::\n\n\nNow that we've updated the model metrics, we can plot model performance over time , again using the `vetiver_plot_metrics()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmonitoring_metrics <-\n  model_board |> pin_read(\"penguin_metrics\")\nvetiver_plot_metrics(monitoring_metrics)\n```\n\n::: {.cell-output-display}\n![](2022-12-30_mlops-azure_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "2022-12-30_mlops-azure_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}