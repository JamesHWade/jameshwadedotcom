{
  "hash": "a388ec1319e140d2fa71eaf8bf39551f",
  "result": {
    "markdown": "---\ntitle: Teaching GPT Models What They Don't Already Know\ndescription: Using vector databases to provide missing context to GPT models\nexecute: \n  eval: false\n---\n\n\nLarge language models like GPT-3 and ChatGPT don't need much of an introduction. They are enormously powerful with benchmarks being , they still struggle to provide accurate results when the appropriate response requires context more recent than the training data for a model. This is where vector databases can be particularly useful.\n\nVector databases allow us to perform semantic search using **embeddings**, which can provide a much more nuanced understanding of language. By creating embeddings of text data and storing them in a database, we can quickly search for related documents and even perform advanced operations like similarity searches or clustering. This can be especially helpful when working with text data that is more context-dependent or domain-specific, such as scientific or technical documentation.\n\n`{gpttools}` provides a set of tools for working with GPT-3 and other OpenAI models, including the ability to generate embeddings, perform similarity searches, and build vector databases. `{gpttools}` also has convenience functions to aid in scraping web pages to collect text data, generate embeddings, and store those embeddings in a vector database for future use.\n\nTo demonstrate the power of vector databases, we'll use `{gpttools}` to build a vector database of the text from [*R for Data Science*](https://r4ds.hadley.nz/). We'll then use the vector database along with GPT-3 to answer questions about writing R code. The approach uses semantic search to find the most relevant text from the book and then uses GPT-3 to generate a response based on that text.\n\n## Scraping Text from R4DS\n\nThe first step is to scrape the text from the R4DS book. We'll use the `crawl()` function to scrape the text from the book and store it in a data frame. The `crawl()` function uses the `rvest` package to scrape the text from the online book and `{tokenizers}` to split the text into chunks for subsequent processing.\n\nThe code to scrape the data is relatively simple:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(gpttools)\ncrawl(\"https://r4ds.hadley.nz/\")\n```\n:::\n\n\nUnder the hood there are a few things going on. Here is the annotated function and associated functions:\n\n::: {.panel-tabset}\n\n## crawl\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncrawl <- function(url) {\n  local_domain <- urltools::url_parse(url)$domain\n  if (!dir.exists(\"text\")) dir.create(\"text\")\n  links <- recursive_hyperlinks(local_domain, url) |> unique()\n  scraped_data <-\n    purrr::map(links, \\(x){\n      if (identical(check_url(x), 200L)) {\n        tibble::tibble(\n          link    = x,\n          text    = paste(scrape_url(x), collapse = \" \"),\n          n_words = tokenizers::count_words(text)\n        )\n      } else {\n        cli::cli_inform(c(\n          \"!\" = \"Skipped {url}\",\n          \"i\" = \"Status code: {status}\"\n        ))\n      }\n    }, .progress = \"Scrape URLs\") |>\n    dplyr::bind_rows() |>\n    dplyr::distinct()\n  arrow::write_parquet(scraped_data,\n                       glue(\"text/{local_domain}.parquet\"))\n}\n```\n:::\n\n\n## recursive_hyperlinks\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrecursive_hyperlinks <- function(local_domain, url, checked_urls = NULL) {\n  links <- url[!(url %in% checked_urls)]\n  if (length(links) < 1) {\n    return(checked_urls)\n  }\n  checked_urls <- c(checked_urls, links)\n  links_df <- purrr::map(links, get_hyperlinks) |>\n    dplyr::bind_rows() |>\n    dplyr::filter(!stringr::str_detect(link,\"^\\\\.$|mailto:|^\\\\.\\\\.|\\\\#|^\\\\_$\"))\n\n  new_links <-\n    purrr::pmap(as.list(links_df), \\(parent, link) {\n      clean_link <- NULL\n      if (stringr::str_detect(link, paste0(\"^https?://\", local_domain))) {\n        clean_link <- link\n      } else if (stringr::str_detect(link, \"^/[^/]|^/+$|^\\\\./|^[[:alnum:]]\") &&\n                 !stringr::str_detect(link, \"^https?://\")) {\n        if (stringr::str_detect(link, \"^\\\\./\")) {\n          clean_link <- stringr::str_replace(link, \"^\\\\./\", \"/\")\n        } else if (stringr::str_detect(link, \"^[[:alnum:]]\")) {\n          clean_link <- glue::glue(\"/\", link)\n        } else {\n          clean_link <- link\n        }\n        clean_link <- glue::glue(\"{parent}{clean_link}\")\n      }\n      validate_link(clean_link, link)\n    }, .progress = \"Collect Links\") |>\n    unlist()\n  recursive_hyperlinks(local_domain, unique(new_links), checked_urls)\n}\n```\n:::\n\n\n## get_hyperlinks\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_hyperlinks <- function(url) {\n  rlang::check_installed(\"rvest\")\n  status <- httr::GET(url) |> httr::status_code()\n  if (identical(status, 200L)) {\n    tibble::tibble(\n      parent = url,\n      link  = rvest::read_html(url) |>\n        rvest::html_nodes(\"a[href]\") |>\n        rvest::html_attr(\"href\") |>\n        unique()\n    )\n  } else {\n    cli::cli_warn(c(\n      \"!\" = \"URL not valid.\",\n      \"i\" = \"Tried to scrape {url}\",\n      \"i\" = \"Status code: {status}\"\n    ))\n  }\n}\n```\n:::\n\n\n## scrape_url\n\n\n::: {.cell}\n\n```{.r .cell-code}\nscrape_url <- function(url) {\n  rlang::check_installed(\"rvest\")\n  exclude_tags <- c(\"style\", \"script\", \"head\", \"meta\", \"link\", \"button\")\n  text <- rvest::read_html(url) |>\n    rvest::html_nodes(xpath = paste(\"//body//*[not(self::\",\n                                    paste(exclude_tags, collapse = \" or self::\"),\n                                    \")]\",\n                                    sep = \"\"\n    )) |>\n    rvest::html_text2() |>\n    remove_new_lines()\n  if (\"You need to enable JavaScript to run this app.\" %in% text) {\n    cli::cli_warn(\"Unable to parse page {url}. JavaScript is required.\")\n    NULL\n  } else {\n    text\n  }\n}\n```\n:::\n\n\n:::\n\n`crawl()` that takes in a single argument, `url`, which is a character string of the URL to be scraped. The function scrapes all hyperlinks within the same domain. The scraped text is processed into a tibble format and saved as a parquet file using the `{arrow}` package into a directory called \"text\" with a filename that includes the local domain extracted earlier.\n\nThe function begins by extracting the local domain of the input URL using the `urltools::url_parse()` function, which returns a parsed URL object, and then extracting the domain component of the object.\n\nThe function then calls another function called `recursive_hyperlinks()` to recursively extract all hyperlinks within the url and validates the links in the process by only keeping urls that return a status code of `200` (i.e., the webpage is accessible).\n\nThe function then loops through each link and scrapes the text from each webpage and creates a tibble with three columns: link, text, and n_words. The link column contains the URL, the text column contains the scraped text, and the n_words column contains the number of words in the scraped text.\n\n## Generating Embeddings\n\nAfter scraping the text data from the R4DS book, the next step is to generate embeddings for each chunk of text. This can be done using the `create_index()` function provided by the `{gpttools}` package. `create_index()` takes in a single argument, `domain`, which should be a character string indicating the domain of the scraped data.\n\nHere's the code to generate embeddings for the R4DS text data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Specify the domain of the scraped data\ndomain <- \"r4ds.hadley.nz\"\n\n# Create embeddings for each chunk of text in the scraped data\nindex <- create_index(domain)\n```\n:::\n\n\n`create_index()` function prepares the scraped data for indexing using the `prepare_scraped_files()` function, which splits the text into chunks and calculates the number of tokens in each chunk. It then calls `add_embeddings()` to generate embeddings for each chunk of text using the OpenAI API. A resulting tibble with embeddings is stored as a feather file using, again using the `{arrow}` package.\n\nHere's the code for the create_index() function and helper functions:\n\n::: {.panel-tabset}\n\n## create_index\n\n::: {.cell}\n\n```{.r .cell-code}\ncreate_index <- function(domain) {\n  index <-\n    prepare_scraped_files(domain = domain) |>\n    add_embeddings()\n  arrow::write_feather(index, sink = glue::glue(\"indices/{domain}.feather\"))\n  index\n}\n```\n:::\n\n\n## prepare_scraped_files\n\n::: {.cell}\n\n```{.r .cell-code}\nprepare_scraped_files <- function(domain) {\n  arrow::read_parquet(glue(\"text/{domain}.parquet\")) |>\n    dplyr::mutate(\n      chunks = purrr::map(text, \\(x) {\n        chunk_with_overlap(x,\n          chunk_size = 500,\n          overlap_size = 50,\n          doc_id = domain,\n          lowercase = FALSE,\n          strip_punct = FALSE,\n          strip_numeric = FALSE,\n          stopwords = NULL\n        )\n      })\n    ) |>\n    tidyr::unnest(chunks) |>\n    tidyr::unnest(chunks) |>\n    dplyr::rename(original_text = text) |>\n    dplyr::mutate(n_tokens = tokenizers::count_characters(chunks) %/% 4)\n}\n```\n:::\n\n\n## add_embeddings\n\n\n::: {.cell}\n\n```{.r .cell-code}\nadd_embeddings <- function(index) {\n  index |>\n    dplyr::mutate(\n      embeddings = purrr::map(\n        .x = chunks,\n        .f = create_openai_embedding,\n        .progress = \"Create Embeddings\"\n      )\n    ) |>\n    tidyr::unnest(embeddings)\n}\n```\n:::\n\n\n## create_openai_embedding\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncreate_openai_embedding <-\n  function(input_text,\n           model = \"text-embedding-ada-002\",\n           openai_api_key = Sys.getenv(\"OPENAI_API_KEY\")) {\n    body <- list(\n      model = model,\n      input = input_text\n    )\n    embedding <- query_openai_api(body, openai_api_key, task = \"embeddings\")\n    embedding$usage$total_tokens\n    tibble::tibble(\n      usage = embedding$usage$total_tokens,\n      embedding = embedding$data$embedding\n    )\n  }\n```\n:::\n\n\n:::\n\nThe resulting tibble contains the following columns:\n\n  - `link`: URL of the webpage\n  - `original_text`: scraped text\n  - `n_words`: number of words in the scraped text\n  - `chunks`: text split into chunks\n  - `usage`: number of tokens used for creating the embedding\n  - `embeddings`: embeddings for each chunk of text\n\n## Querying with Embeddings\n\nAfter generating embeddings for each chunk of text, the next step is to query the embeddings to find similar chunks of text. This can be done using the `query_index()` function provided by the `{gpttools}` package. This is a bit of a complicated function, so it's worth taking a look at the code to see how it works.\n\n::: {.panel-tabset}\n\n## query_index\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquery_index <- function(index, query, task = \"conservative q&a\", k = 4) {\n  arg_match(\n    task,\n    c(\n      \"conservative q&a\", \"permissive q&a\",\n      \"paragraph about a question\", \"bullet points\",\n      \"summarize problems given a topic\",\n      \"extract key libraries and tools\",\n      \"simple instructions\", \"summarize\"\n    )\n  )\n  query_embedding <- create_openai_embedding(input_text = query) |>\n    dplyr::pull(embedding) |>\n    unlist()\n\n  full_context <- get_top_matches(index, query_embedding, k = k)\n\n  context <-\n    full_context |>\n    dplyr::pull(chunks) |>\n    paste(collapse = \"\\n\\n\")\n\n  instructions <-\n    switch(task,\n      \"conservative q&a\" =\n        glue::glue(\n          \"Answer the question based on the context below, and if the question\n          can't be answered based on the context, say \\\"I don't know\\\"\\n\\n\n          Context:\\n{context}\\n\\n---\\n\\nQuestion: {query}\\nAnswer:\"\n        ),\n      \"permissive q&a\" =\n        glue::glue(\n          \"Answer the question based on the context below, and if the question\n          can't be answered based on the context, say \\\"This is a tough\n          question but I can answer anyway.\\\"\\n\\n\n          Context:\\n{context}\\n\\n---\\n\\nQuestion: {query}\\nAnswer:\"\n        ),\n      \"paragraph about a question\" =\n        glue::glue(\n          \"Write a paragraph, addressing the question, and use the text below\n          to obtain relevant information\\\"\\n\\nContext:\\n\n          {context}\\n\\n---\\n\\nQuestion: {query}\\nParagraph long Answer:\"\n        ),\n      \"bullet points\" =\n        glue::glue(\n          \"Write a bullet point list of possible answers, addressing the\n          question, and use the text below to obtain relevant information\\\"\\n\\nC\n          ontext:\\n{context}\\n\\n---\\n\\nQuestion: {query}\\nBullet point\n          Answer:\"\n        ),\n      \"summarize problems given a topic\" =\n        glue::glue(\n          \"Write a summary of the problems addressed by the questions below\\\"\\n\n          \\n{context}\\n\\n---\\n\\n\"\n        ),\n      \"extract key libraries and tools\" =\n        glue::glue(\"Write a list of libraries and tools present in the context\n                   below\\\"\\n\\nContext:\\n{context}\\n\\n---\\n\\n\"),\n      \"simple instructions\" =\n        glue::glue(\"{query} given the common questions and answers below \\n\\n\n                   {context}\\n\\n---\\n\\n\"),\n      \"summarize\" =\n        glue::glue(\"Write an elaborate, paragraph long summary about\n                   \\\"{query}\\\" given the questions and answers from a public\n                   forum or documentation page on this topic\\n\\n{context}\\n\\n\n                   ---\\n\\nSummary:\"),\n    )\n\n  n_tokens <- tokenizers::count_characters(instructions) %/% 4\n  if (n_tokens > 3500) {\n    answer <-\n      list(\n        choice = list(\n          text = \"Too many tokens. Please lower the number of documents (k).\"\n        )\n      )\n  } else {\n    answer <- openai_create_completion(\n      model = \"text-davinci-003\",\n      prompt = instructions,\n      max_tokens = as.integer(3800L - n_tokens)\n    )\n  }\n  list(instructions, full_context, answer)\n}\n```\n:::\n\n\n## get_top_matches\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_top_matches <- function(index, query_embedding, k = 5) {\n  index |>\n    dplyr::rowwise() |>\n    dplyr::mutate(similarity = lsa::cosine(\n      query_embedding,\n      embedding |> unlist()\n    )) |>\n    dplyr::arrange(desc(similarity)) |>\n    head(k)\n}\n```\n:::\n\n\n## openai_create_completion\n\n\n::: {.cell}\n\n```{.r .cell-code}\nopenai_create_completion <-\n  function(model = \"text_davinci-003\",\n           prompt = \"<|endoftext|>\",\n           suffix = NULL,\n           max_tokens = 16,\n           temperature = NULL,\n           top_p = NULL,\n           openai_api_key = Sys.getenv(\"OPENAI_API_KEY\")) {\n    assert_that(\n      is.string(model),\n      is.string(prompt),\n      is.count(max_tokens),\n      is.string(suffix) || is.null(suffix),\n      value_between(temperature, 0, 1) || is.null(temperature),\n      is.string(openai_api_key),\n      value_between(top_p, 0, 1) || is.null(top_p)\n    )\n\n    if (is.number(temperature) && is.number(top_p)) {\n      cli_warn(\"Specify either temperature or top_p, not both.\")\n    }\n\n    body <- list(\n      model = model,\n      prompt = prompt,\n      suffix = suffix,\n      max_tokens = max_tokens,\n      temperature = temperature\n    )\n\n    query_openai_api(body, openai_api_key, task = \"completions\")\n  }\n```\n:::\n\n\n:::\n\nThe `query_index()` function takes in four arguments: `index`, `query`, `task`, and `k`.\n\n* `index`: The index containing the embeddings to be queried.\n* `query`: The query string to search for similar chunks of text.\n* `task`: The type of task to perform based on the context of the query. {gpttools} provides a few pre-defined tasks, such as \"conservative q&a\" or \"extract key libraries and tools\", which can be passed to this argument. These queries were taken from a repo created by OpenAI and Pinecone, which can be found [here](https://github.com/pinecone-io/examples/tree/master/integrations/openai/beyond_search_webinar).\n* `k`: The number of similar chunks of text to return.\n\nThe function generates an embedding for the query string with `create_openai_embedding()`. It then uses the `get_top_matches()` to find the most similar chunks of text in the index using cosine similarity returning the top `k` matches.\n\nThe next step is to formats the instructions based on the `task` argument as well as the `query` and `context`. For example, if the task is \"conservative q&a\", the function will return a string asking the model to answer a question based on the context of the returned chunks of text. If the task is \"extract key libraries and tools\", the function will return a string listing the libraries and tools present in the returned chunks of text.\n\nThe prompt is passed to OpenAI's GPT-3 `text-davince-003` to generate a response based on the formatted output. The response is returned as a list containing the instructions, the top `k` matches, and the response from OpenAI's GPT-3.\n\n## Analysis of Embeddings with tidymodels and UMAP\n\nThe next step is to perform dimesnion reduction with UMAP as naive exploration of the embeddigns. The `embeddings` column in the index contains a list of 1536-dimensional vectors. We can use the `recipes` package to normalize the vectors and then use the `step_umap()` function to reduce the dimensionality to 2. We can then plot the results.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(recipes)\nlibrary(ggplot2)\nlibrary(purrr)\nlibrary(tidyr)\nlibrary(embed)\n\nindex_wide <-  \nindex |>\n  mutate(embedding = map(embedding, \\(x) as.data.frame(t(x)))) |>\n  unnest(embedding)\n\nset.seed(123)\n\numap_spec <- recipe(~ ., data = index_wide) |>\n  step_normalize(starts_with(\"V\")) |>\n  step_umap(starts_with(\"V\"), num_comp = 2)\n\numap_estimates <- prep(umap_spec, training = index_wide)\numap_data <- bake(umap_estimates, new_data = NULL)\n\numap_data |> \n  ggplot() +\n  geom_point(aes(x = UMAP1, y = UMAP2, color = link),\n             alpha = 0.5, size = 2, show.legend = FALSE) +\n  labs(title = \"Dimensionality Reduction with UMAP\",\n       subtitle = \"UMAP of 1536-dimensional vectors | Colored by Source Text\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](2023-03-10_vectorstores_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::",
    "supporting": [
      "2023-03-10_vectorstores_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}