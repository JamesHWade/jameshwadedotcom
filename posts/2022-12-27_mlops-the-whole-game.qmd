---
title: "MLOps: The Whole Game"
author: James H Wade
date: 2022-12-27
thanks: Thank you to the MLOps and tidymodels teams at Posit and all the open source developers that contributed to this fantastic ecosystem.
description: An example of model building, model delpoyment, and model monitoring with R using palmerpenguins
execute: 
  freeze: true
format: 
  html:
    toc: true
    code-copy: true
image: images/penguins.png
categories: 
  - mlops
  - modeling
  - vetiver
  - pins
  - deployment
  - R
---

> If we actually know what we're doing we call it engineering. So, this basket of pain, we call something ops.
>
> <cite>Peter Wang, [Numerically Speaking Podcast](https://www.anaconda.com/podcast/human-in-the-loop) with Vicky Boykis on ML & MLOps</cite>

MLOps can be overwhelming, but that is not the way it has to be. Posit's MLOps team has made from fantastic advancements over the past year or so, and I hope to show how to demonstrate how easy model deployment can be using Posit's open source tools for MLOps. This includes `{pins}`, `{vetiver}`, and the `{tidymodels}` bundle of packages along with the `{tidyverse}`. The motivation for this post came in part from a [Numerically Speaking podcast](https://www.anaconda.com/podcast/human-in-the-loop) I quote above, and much of the model building is taken from Julia Silge's blog post written to help R users [get started with tidymodels](https://juliasilge.com/blog/palmer-penguins/). I also found inspiration from the [`{vetiver}` documentation page](https://vetiver.rstudio.com/) and the recently revamped [Solutions Engineering Page from Posit](https://solutions.posit.co/gallery/bike_predict/).

The post covers most of steps in MLOps process, but it's more of a sampler than exhaustive coverage. Think of this as the ["whole game"](https://r-pkgs.org/whole-game.html) of MLOps with R.

[![Source: MLOps Team at Posit \| An overview of MLOps with Vetiver and friends](images/vetiver-mlops.png){fig-alt="During the MLOps cycle, we collect data, understand and clean the data, train and evaluate a model, deploy the model, and monitor the deployed model. Monitoring can then lead back to collecting more data. There are many great tools available to understand clean data (like pandas and the tidyverse) and to build models (like tidymodels and scikit-learn). Use the vetiver framework to deploy and monitor your models." fig-align="center" width="700"}](https://vetiver.rstudio.com/)

## Model Building

### Load Packages and Set Options

Let's start with the packages that we'll use throughout. `{tidyverse}` and `{tidymodels}` are there, of course. `{pins}`, `{plumbr}`, and `{vetiver}` completes the rest of the Posit set for MLOps, and I use `{gt}` for tables.

```{r load-pkg}
#| message: false
library(tidyverse)
library(tidymodels)
library(pins)
library(vetiver)
library(plumber)
library(palmerpenguins)
library(gt)
library(conflicted)
tidymodels_prefer()
conflict_prefer("penguins", "palmerpenguins")
theme_set(theme_bw())
options(tidymodels.dark = TRUE)
```

### Data Exploration

For this example, we'll use the `palmerpenguins` dataset to demonstrate the overall approach. There is a `{palmerpenguins}` package that contains this data set, and it is also included in the `{modeldata}` package, a part of `{tidymodels}`. We'll use the data from `{palmerpenguins}`.

```{r}
#| code-fold: true
penguins |>
  head(4) |>
  glimpse()
```

As Julia's [post points out](https://juliasilge.com/blog/palmer-penguins/), differentiating the species with a classification model is quite easy. A trickier model is one that predicts the penguin sex. Let's look at a plot of `flipper_length_mm` versus `bill_length_mm` for each of the species. The color indicates `sex` and the point size indicates `body_mass_g`.

```{r}
#| code-fold: true
penguins %>%
  filter(!is.na(sex)) %>%
  ggplot(aes(
    x = flipper_length_mm,
    y = bill_length_mm,
    color = sex,
    size = body_mass_g
  )) +
  geom_point(alpha = 0.5) +
  facet_wrap(~species)
```

### Model Splitting & Bootstrapping

Let's do a little data cleaning before we move onto modeling. This will include removing any missing `sex` assignments and removing `year` and `island` columns.

```{r}
penguins_df <-
  penguins |>
  drop_na(sex) |>
  select(-year, -island)
```

The `{tidymodels}` ecosystem has convenience functions for data splitting that help us do the "right" thing during model building. The default split between training and testing set is 75:25.

```{r}
set.seed(1234)
penguin_split <- initial_split(penguins_df, strata = sex)
penguin_train <- training(penguin_split)
penguin_test <- testing(penguin_split)
```

### Preprocess with `{recipes}`

For preprocessing of the data, let's use `{recipes}`.

```{r}
penguin_rec <-
  recipe(sex ~ ., data = penguin_train) |>
  step_YeoJohnson(all_numeric_predictors()) |>
  step_normalize(all_numeric_predictors()) |>
  step_dummy(species)
```

The `penguin_rec` recipe is a process for preparing data for modeling. It consists of four steps:

1.  The `recipe()` function creates a recipe object, which is a sequence of steps for preprocessing data. The first argument to the function specifies the outcome variable (`sex`) and the predictor variables (`.`, which stands for all variables in the data). The `data` argument specifies the data frame to use for the recipe.

2.  The `step_YeoJohnson()` function applies a Yeo-Johnson transformation to all numeric predictors in the data. This transformation is a type of power transformation that can help normalize data by making it more symmetric and reducing the influence of outliers.

3.  The `step_normalize()` function normalizes all numeric predictors in the data. Normalization scales the data so that it has a mean of 0 and a standard deviation of 1.

4.  The `step_dummy()` function creates dummy variables for the `species` variable. Dummy variables are binary variables that are used to represent categorical variables in a regression model.

Overall, this recipe applies several preprocessing steps to the data in order to prepare it for modeling. The transformed and normalized data, along with the dummy variables, can then be used to build a predictive model.

### Specify the Model

We'll evaluate three modeling approaches. In the code below, `glm_spec`, `tree_spec`, and `mlp_brulee_spec` are specifications for three different machine learning models: a logistic regression model, a random forest model, and a multi-layer perceptron (MLP) model. The intent with model selection was to demonstrate the use of very different models rather than pick an ideal set of models to screen.

The `logistic_reg()` function creates a specification for a logistic regression model, and the `set_engine('glm')` function sets the engine for the model to be `'glm'`, which stands for generalized linear model.

The `rand_forest()` function creates a specification for a random forest model, and the `set_engine('ranger')` function sets the engine for the model to be `'ranger'`, which is an implementation of random forests using the `{ranger}` package. The `set_mode('classification')` function sets the mode of the model to be classification. `set_mode()` is not needed for logistic regression as that model is only used for classification. (Yes, the name is a bad one for what it does.)

The `mlp()` function creates a specification for an MLP model, and the `set_engine('brulee')` function sets the engine for the model to be `'brulee'`, which uses {`torch}` to specify and fit the neural network. The `tune()` function indicates that the hyperparameters for the model (`hidden_units`, `epochs`, `penalty`, and `learn_rate`) should be tuned.

```{r}
glm_spec <-
  logistic_reg() |>
  set_engine("glm")

tree_spec <-
  rand_forest(min_n = tune()) |>
  set_engine("ranger") |>
  set_mode("classification")

mlp_brulee_spec <-
  mlp(
    hidden_units = tune(), epochs = tune(),
    penalty = tune(), learn_rate = tune()
  ) %>%
  set_engine("brulee") %>%
  set_mode("classification")
```

### Create the Workflow Set and Fit Models

Before we fit the models specified above, let's use cross validation for more robust model evaluation and set the parameters for hyperparameter tuning.

The `set.seed()` function sets the seed for the random number generator, which is helps improve the reproducibility of the code.

The `vfold_cv()` function creates a v-fold cross-validation object, which is used to evaluate the performance of a model on different subsets of the data. The `penguin_folds` object stores the folds that will be used for cross-validation.

The `control_bayes()` creates an object to store the settings for Bayesian optimization. Bayesian optimization is a method for finding the optimal set of hyperparameters for a machine learning model. The `no_improve` argument specifies the number of consecutive iterations with no improvement in the objective function before the optimization process is terminated. The `time_limit` argument specifies the maximum amount of time that the optimization process can run in minutes. The `save_pred` argument specifies whether to save the predictions made during the optimization process.

```{r}
set.seed(1234)
penguin_folds <- vfold_cv(penguin_train)

bayes_control <-
  control_bayes(no_improve = 10L, time_limit = 20, save_pred = TRUE)
```

A workflow set combines the recipes and models to fit to our training data. The `{workflowsets}` package is an extension of the `{workflow}` package that allows us to evaluate multiple preprocessing and modeling approaches all together. The `workflow_set()` function creates a workflow set object, which consists of a list of preprocessing recipes in the `preproc` argument and a list of modeling specifications in the `models` argument.

The `workflow_map()` function applies a function to each element of the workflow set. In this case, we use the `tune_bayes` function, which performs Bayesian optimization using the `{tune}` package. The `iter` argument specifies the maximum number of iterations for each model, the `resamples` argument specifies the cross-validation folds to use, and the `control` argument specifies the settings for Bayesian optimization that we defined above.

Overall, this code creates a workflow set consisting of three models (a logistic regression model, a random forest model, and an MLP model) with preprocessing steps applied to the data, and then performs Bayesian optimization to tune the hyperparameters of the models using cross-validation.[^1]

[^1]: For ease of compute, I don't actually re-calculate the workflow_set in this document. There's a hidden chunk that reads a pinned version of the workflow_set result. For the curious, that's `model_board <- board_local()` and `workflow_set <- mode_board |> pin_read("workflow_set")` .

```{r}
#| eval: false
workflow_set <-
  workflow_set(
    preproc = list(penguin_rec),
    models = list(
      glm = glm_spec,
      tree = tree_spec,
      torch = mlp_brulee_spec
    )
  ) |>
  workflow_map("tune_bayes",
    iter = 50L,
    resamples = penguin_folds,
    control = bayes_control
  )
```

```{r}
#| include: false
model_board <- board_local()
# workflow_set <- model_board |> pin_write(workflow_set)
workflow_set <- model_board |> pin_read("workflow_set")
```

We can now use `rank_results()` to rank the models in the workflow set based on their performance based on our specified metrics - the area under the receiver operating characteristic curve (ROC AUC). ROC AUC is a measure of a model's ability to distinguish between positive and negative classes. A higher ROC AUC indicates a better-performing model with a maximum value of 1. Using the rank table, we can select the workflow ID for the best performing model.

Throughout many tidymodels packages, `autoplot` is a handy method to rapidly visualize steps in a model workflow. These methods are specified by the package authors, and some `autoplot` methods have some options to customize the output. These are `ggplot` objects, so customize their appearance is easy.

```{r}
rank_results(workflow_set,
  rank_metric = "roc_auc",
  select_best = TRUE
) |>
  gt()

workflow_set |> autoplot()
```

In this case `autoplot()` compare the results from each of our workflows showing both `accuracy` and `roc_auc`. Logistic regression appears to be the best model based on these metrics given its comparable performance and lower model complexity.

```{r}
best_model_id <- "recipe_glm"
```

### Finalize Model

Now that we have compared our models and identified the top performing one based on `roc_auc`, we can finalize the workflow and fit the model will the full dataset (i.e., not just training data).

In the code below, the `best_fit` object is extract the best model from the workflow using the workflow ID we selected above. This is done with `workflowsets::extract_workflow_set_result()` and `tune::select_best()` to give us `best_fit`, a tibble of hyperparameters for the best fit model.

We can then use `finalize_workflow()` to take the hyperparameters from `best_fit` and apply it to the `final_workflow` object. We can then update the fit of the model to use the entire training set instead of folds and evaluate the model on the test set.

The `collect_metrics()` and `collect_performance()` functions are convenience functions to to check model performance. We can again use `autoplot()` to visualize model results, in this case ROC curves.

```{r}
best_fit <-
  workflow_set |>
  extract_workflow_set_result(best_model_id) |>
  select_best(metric = "accuracy")

final_workflow <-
  workflow_set |>
  extract_workflow(best_model_id) |>
  finalize_workflow(best_fit)

final_fit <-
  final_workflow |>
  last_fit(penguin_split)

final_fit |>
  collect_metrics() |>
  gt()

final_fit |>
  collect_predictions() |>
  roc_curve(sex, .pred_female) |>
  autoplot()
```

## Model Deployment

The [`{vetiver}`](https://rstudio.github.io/vetiver-r/) package provides a set of tools for building, deploying, and managing machine learning models in production. It allows users to easily create, version, and deploy machine learning models to various hosting platforms, such as Posit Connect or a cloud hosting service like Azure.

The `vetiver_model()` function is used to create an object that stores a machine learning model and its associated metadata, such as the model's name, type, and parameters. `vetiver_pin_write()` and `vetiver_pin_read()` functions are used to write and read `vetiver_model` objects to and from a server.

### Create Vetiver Model

To deploy our model with `{vetiver}`, we start with our `final_fit` from above, we first need to extract the trained workflow. We can do that with `tune::extract_workflow()`. The trained workflow is what we will deploy as a `vetiver_model`. That means we need to convert it from a workflow to a vetiver model with `vetiver_model()`.

```{r}
final_fit_to_deploy <- final_fit |> extract_workflow()

v <- vetiver_model(final_fit_to_deploy, model_name = "penguins_model")

v
```

### Pin Model to Board

The [`{pins}`](https://pins.rstudio.com/) package is used for storing and managing data sets in a local or remote repository. `{pins}` allows users to "pin" data sets to a "board", allowing them to be easily accessed and shared with others. Using the pins package, users can create a board, add data sets, and access and retrieve data sets from the board. The `board_rsconnect()` function is used to create a `model_board` or connect to an existing board on Posit Connect (formerly RStudio Connect), which is a connection to a server where a `vetiver_model` can be stored and accessed. We also specify `versioned = TRUE` so that we can version control our vetiver models.

Once the `model_board` connection is made it's as easy as `vetiver_pin_write()` to "pin" our model to the model board and `vetiver_pin_read()` to access it. In this case, we must specify the username of the author of the pin, which in this case is `james`.

#### Local Board

For the purpose of this post, we using a local board with the `board_local()` function for writing and reading pins. If our goal is to reproduce the resutls in this post, it will do, but that defeats the purpose of deployment since we cannot share the pin with others.

```{r}
model_board <- board_local(versioned = TRUE)
model_board |> vetiver_pin_write(v)
model_board |> vetiver_pin_read("penguins_model")
```

#### Posit Connect Deployment

Combining Posit's open source tools with their professions products is quite a powerful combination. Deploying to Posit connect is nearly the same as using a local board. They key difference is specifying an account name when reading the pin. In this case, we use `james` to indicate the account name, but this will be specific to the Posit Connect configuration.

```{r}
#| eval: false
model_board <- board_rsconnect(versioned = TRUE)
model_board |> vetiver_pin_write(v)
model_board |> vetiver_pin_read("james/penguins_model")
```

#### Deploying Elsewhere

If you do not have access to Posit Connect or prefer a different deployment location, please let me know. I plan to cover it in future posts. Commenting on this post would be a great way to make that suggestion. I will start with Azure since that is what I use professionally.

### Create Model API

Our next step is to use `{vetiver}` and [`{plumber}`](https://www.rplumber.io/) packages to create an API for our vetiver model, which can then be accessed and used to make predictions or perform other tasks via an HTTP request. `pr()` creates a new plumber router, and `vetiver_api(v)` adds a `POST` endpoint to make endpoints from a trained vetiver model. `vetiver_write_plumber()` creates a `plumber.R` file that specifies the model version of the model we pinned to our model dashboard with `vetiver_pin_write()`.

```{r}
pr() |>
  vetiver_api(v)

vetiver_write_plumber(model_board, "penguins_model")
```

Here is an example of the `plumber.R` file generated by `vetiver_write_pumber()`.

```{r}
#| eval: false
# Generated by the vetiver package; edit with care

library(pins)
library(plumber)
library(rapidoc)
library(vetiver)

# Packages needed to generate model predictions
if (FALSE) {
  library(parsnip)
  library(recipes)
  library(stats)
  library(workflows)
}
b <- board_rsconnect("envvar", server = "https://connect.mycompany.com")
v <- vetiver_pin_read(b, "penguins_model", version = "6926")

#* @plumber
function(pr) {
  pr %>% vetiver_api(v)
}
```

### Deploy API to Posit Connect

This model can be hosted in a variety of locations. One of the easiest to use is Posit Connect. `vetiver_deploy_rsconnect()` does that for us. All we need to specify is the name of the pinned vetiver model and the model board.

```{r}
#| eval: false
vetiver_deploy_rsconnect(
  board = model_board,
  name = "penguins_model",
  account = "james"
)
```

### Deploying Elsewhere

If Posit Connect is not the right place for our model, `vetiver_write_docker` creates a `dockerfile` and `renv.lock`.

```{r}
#| eval: false
vetiver_write_docker(v)
```

Here is an example of the dockerfile that is generated.

``` dockerfile
# Generated by the vetiver package; edit with care

FROM rocker/r-ver:4.2.1
ENV RENV_CONFIG_REPOS_OVERRIDE https://packagemanager.rstudio.com/cran/latest

RUN apt-get update -qq && apt-get install -y --no-install-recommends \
libcurl4-openssl-dev \
libicu-dev \
libsodium-dev \
libssl-dev \
make \
zlib1g-dev \
&& apt-get clean

COPY vetiver_renv.lock renv.lock
RUN Rscript -e "install.packages('renv')"
RUN Rscript -e "renv::restore()"
COPY plumber.R /opt/ml/plumber.R
EXPOSE 8000
ENTRYPOINT ["R", "-e", "pr <- plumber::plumb('/opt/ml/plumber.R'); pr$run(host = '0.0.0.0', port = 8000)"]
```

### Using the API to Make Predictions

The api deployment site url `https://connect.mycompany.com/penguins`, and the prediction endpoint is `https://connect.mycompany.com/penguins/predict`.

```{r}
#| eval: false
endpoint <-
  vetiver_endpoint("https://connect.mycompany.com/penguins/predict")
```

We can make endpoints with the endpoint using `predict`.

```{r}
#| eval: false
new_data <- tibble(
  species = "Adelie",
  bill_length_mm = 40.5,
  bill_depth_mm = 18.9,
  flipper_length_mm = 180,
  body_mass_g = 3950
)

predict(endpoint, new_data)
```

We can also use `{httr}` to call the API. In most cases, it is easier for R users to use `predict` rather than `httr::POST`. However, were this model written in another language, making predictions using `{httr}` would likely bet the best approach.

```{r}
#| eval: false
library(httr)
url <- "https://connect.mycompany.com/penguins/predict"
json_data <- jsonlite::toJSON(new_data)
response <- POST(url, body = json_data)
response
content(response)
```

Avoiding a language-specific approach altogether, we can use `curl` in a terminal to make API calls.

``` {.bash filename="Terminal"}
#| file
curl -X POST "https://connect.mycompany.com/penguins/predict" \
 -H "Accept: application/json" \
 -H "Content-Type: application/json" \
 -d '[{"species":"Adelie","bill_length_mm":0.5,"bill_depth_mm":0.5,"flipper_length_mm":0,"body_mass_g":0}]' \
```

## Model Monitoring

After deployment, we need to monitor model performance. The [MLOps with vetiver monitoring page](https://vetiver.rstudio.com/get-started/monitor.html) describes this well:

> Machine learning can break quietly; a model can continue returning predictions without error, even if it is performing poorly. Often these quiet performance problems are discussed as types of model drift; data drift can occur when the statistical distribution of an input feature changes, or concept drift occurs when there is change in the relationship between the input features and the outcome.
>
> Without monitoring for degradation, this silent failure can continue undiagnosed. The vetiver framework offers functions to fluently compute, store, and plot model metrics. These functions are particularly suited to monitoring your model using multiple performance metrics over time. Effective model monitoring is not "one size fits all", but instead depends on choosing appropriate metrics and time aggregation for a given application.

As a baseline for model performance, we can start by using our training set to create original metrics for the model. We also simulate a `date_obs` column. In a real example, we should use the date the data was collected.

```{r}
set.seed(1234)
penguin_train_by_date <-
  penguin_train |>
  rowwise() |>
  mutate(date_obs = Sys.Date() - sample(4:10, 1)) |>
  ungroup() |>
  arrange(date_obs)

original_metrics <-
  augment(v, penguin_train_by_date) |>
  vetiver_compute_metrics(
    date_var = date_obs,
    period = "day",
    truth = "sex",
    estimate = ".pred_class"
  )

vetiver_plot_metrics(original_metrics)
```

We can pin the model performance metrics, just as we did with the model.

```{r}
model_board %>%
  pin_write(original_metrics, "penguin_metrics")
```

To simulate the model going "live", let's use the test set to add more predictions.

```{r}
set.seed(1234)
penguin_test_by_date <-
  penguin_test |>
  rowwise() |>
  mutate(date_obs = Sys.Date() - sample(1:3, 1)) |>
  ungroup() |>
  arrange(date_obs)

v <-
  model_board |>
  vetiver_pin_read("penguins_model")

new_metrics <-
  augment(v, penguin_test_by_date) |>
  vetiver_compute_metrics(
    date_var = date_obs,
    period = "day",
    truth = "sex",
    estimate = ".pred_class"
  )

model_board |>
  vetiver_pin_metrics(new_metrics, "penguin_metrics")
```

Now that we've updated the model metrics, we can plot model performance over time , again using the `vetiver_plot_metrics()` function.

```{r}
monitoring_metrics <-
  model_board |> pin_read("penguin_metrics")
vetiver_plot_metrics(monitoring_metrics)
```
